{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10580423,"sourceType":"datasetVersion","datasetId":6547867},{"sourceId":10580457,"sourceType":"datasetVersion","datasetId":6547892},{"sourceId":10910263,"sourceType":"datasetVersion","datasetId":6772165},{"sourceId":11054446,"sourceType":"datasetVersion","datasetId":6887071},{"sourceId":11102308,"sourceType":"datasetVersion","datasetId":6876345},{"sourceId":228567219,"sourceType":"kernelVersion"},{"sourceId":229479655,"sourceType":"kernelVersion"},{"sourceId":229496187,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":3908.758451,"end_time":"2025-03-20T11:39:54.811719","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-20T10:34:46.053268","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Urban Heat Island (UHI) Prediction Notebook\n\nThis notebook details an end-to-end process for modeling Urban Heat Island (UHI) effects. We leverage continuous numerical features extracted from multiple datasets, including:\n\n- **Sentinel-2 & Landsat imagery**\n- **Weather data**\n- **Building footprints (shp & kml)**\n- **Elevation**\n- **Traffic**\n- **Street tree census**\n- **Forestry**\n- **OSMnx derived urban features**\n\nDatasets not included:\n\n- **Air pollution raster**\n- **Heat vulnerability index**\n\nAll of these datasets contribute to the urban heat effect and when combined create strong predictive features for modeling.\n\nThis methodology uses out-of-fold (OOF) ensembling with diverse ExtraTreesRegressor models, followed by a Ridge Kernel meta-model to blend the predictions.\n\n---","metadata":{}},{"cell_type":"code","source":"print(f\"numpy: {np.__version__}\")\nprint(f\"pandas: {pd.__version__}\")\nprint(f\"joblib: {joblib.__version__}\")\nprint(f\"scikit-learn: {KFold.__module__.split('.')[0]}\")  # Single sklearn version check\nprint(f\"gc: {gc.__version__ if hasattr(gc, '__version__') else 'Built-in (no version)'}\")\nprint(f\"os: Built-in Python module (no version)\")\n\n# For sklearn submodules (all share same version)\nimport sklearn\nsklearn_version = KFold.__module__.split('.')[0]\nprint(f\"\\nAll scikit-learn components use version: {sklearn.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T00:34:39.172419Z","iopub.execute_input":"2025-03-26T00:34:39.172806Z","iopub.status.idle":"2025-03-26T00:34:39.180047Z","shell.execute_reply.started":"2025-03-26T00:34:39.172776Z","shell.execute_reply":"2025-03-26T00:34:39.179144Z"}},"outputs":[{"name":"stdout","text":"numpy: 1.26.4\npandas: 2.2.3\njoblib: 1.4.2\nscikit-learn: sklearn\ngc: Built-in (no version)\nos: Built-in Python module (no version)\n\nAll scikit-learn components use version: 1.2.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport joblib\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.base import clone\nimport gc\nfrom sklearn.ensemble import ExtraTreesRegressor\n# , RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n# import xgboost as xgb\n# import lightgbm as lgb\n# import catboost as cb\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.neural_network import MLPRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n# from sklearn.linear_model import ElasticNet, Lasso, Ridge\n# from sklearn.svm import SVR\n# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n# from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n# from sklearn.neighbors import KNeighborsRegressor\n# from scipy.stats import boxcox\n# from scipy.special import inv_boxcox\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n# import tensorflow as tf\n# from sklearn.svm import SVR\n# from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n# from sklearn.preprocessing import QuantileTransformer\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-26T00:33:51.794046Z","iopub.execute_input":"2025-03-26T00:33:51.794307Z","iopub.status.idle":"2025-03-26T00:33:55.521624Z","shell.execute_reply.started":"2025-03-26T00:33:51.794279Z","shell.execute_reply":"2025-03-26T00:33:55.520497Z"},"papermill":{"duration":1.053544,"end_time":"2025-03-20T10:34:50.192415","exception":false,"start_time":"2025-03-20T10:34:49.138871","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## 2. Data Loading\n\nThis section imports training and test datasets, along with additional CSV files containing ensemble model predictions and complete data. The UHI Index serves as the target variable.\n\n> **Note:** Data is loaded directly from the Kaggle input directory.\n\n---","metadata":{}},{"cell_type":"code","source":"%time\ntrain_df = pd.read_csv('/kaggle/input/uhi-index-data/Training_data_uhi_index_UHI2025-v2.csv')\ntest_df = pd.read_csv('/kaggle/input/test-df-ey-open-science/Submission_template_UHI2025-v2.csv')\n# train_peak = pd.read_csv('/kaggle/input/0-9770-cv-oof-preds-21-etrs-10f/final_boss_cv_0.9771.csv')\n# test_peak = pd.read_csv('/kaggle/input/0-9770-cv-oof-preds-21-etrs-10f/final_test_cv_0.9771.csv')\ntrain_com = pd.read_csv('/kaggle/input/ey-complete-data/final_boss_complete_data.csv')\ntest_com = pd.read_csv('/kaggle/input/ey-complete-data/final_test_complete_data.csv')\ntarget = train_df['UHI Index']","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:16:59.380615Z","iopub.execute_input":"2025-03-23T15:16:59.380950Z","iopub.status.idle":"2025-03-23T15:17:02.995247Z","shell.execute_reply.started":"2025-03-23T15:16:59.380923Z","shell.execute_reply":"2025-03-23T15:17:02.994212Z"},"papermill":{"duration":7.739606,"end_time":"2025-03-20T10:34:57.947548","exception":false,"start_time":"2025-03-20T10:34:50.207942","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preprocessing & Cleaning\n\n- **Missing Values:** Missing values are primarily found in buffer regions where there was no intersection. In this context, these NaNs are imputed as zeros.\n\n---","metadata":{}},{"cell_type":"code","source":"test_nulls = ['building_area_min_1000m', 'building_area_median_1000m',\n       'building_height_median_1000m', 'ground_ele_mean_1000m',\n       'ground_ele_min_1000m', 'ground_ele_std_1000m',\n       'building_area_median_1000m_ring', 'ground_ele_mean_1000m_ring',\n       'ground_ele_std_1000m_ring', 'building_area_min_500m',\n       'building_area_median_500m', 'building_area_std_500m',\n       'building_height_max_500m', 'building_height_median_500m',\n       'ground_ele_min_500m', 'building_area_std_500m_ring',\n       'building_height_max_500m_ring', 'building_area_min_400m',\n       'building_height_max_400m', 'ground_ele_std_400m',\n       'building_height_max_400m_ring', 'ground_ele_std_400m_ring',\n       'ground_ele_mean_300m', 'ground_ele_median_300m', 'ground_ele_std_300m',\n       'building_height_std_250m', 'ground_ele_mean_250m',\n       'ground_ele_mean_250m_ring', 'building_height_max_150m',\n       'ground_ele_median_150m', 'building_height_max_150m_ring',\n       'ground_ele_median_150m_ring', 'ground_ele_median_100m',\n       'building_area_std_1000m_kml', 'building_area_min_1000m_kml_ring',\n       'building_area_median_1000m_kml_ring',\n       'building_area_std_1000m_kml_ring', 'building_area_min_500m_kml',\n       'building_area_median_500m_kml', 'building_area_std_500m_kml',\n       'building_area_median_500m_kml_ring', 'building_area_min_400m_kml',\n       'building_area_std_400m_kml', 'building_area_min_400m_kml_ring',\n       'building_area_std_300m_kml', 'building_area_min_250m_kml',\n       'building_area_std_250m_kml_ring']\n\ntrain_nulls = ['building_area_min_1000m', 'building_area_median_1000m',\n       'building_height_median_1000m', 'ground_ele_mean_1000m',\n       'ground_ele_min_1000m', 'ground_ele_std_1000m',\n       'building_area_median_1000m_ring', 'ground_ele_mean_1000m_ring',\n       'ground_ele_std_1000m_ring', 'building_area_min_500m',\n       'building_area_median_500m', 'building_area_std_500m',\n       'building_height_max_500m', 'building_height_median_500m',\n       'ground_ele_min_500m', 'building_area_std_500m_ring',\n       'building_height_max_500m_ring', 'building_area_min_400m',\n       'building_height_max_400m', 'ground_ele_std_400m',\n       'building_height_max_400m_ring', 'ground_ele_std_400m_ring',\n       'ground_ele_mean_300m', 'ground_ele_median_300m', 'ground_ele_std_300m',\n       'building_height_std_250m', 'ground_ele_mean_250m',\n       'ground_ele_mean_250m_ring', 'building_height_max_150m',\n       'ground_ele_median_150m', 'building_height_max_150m_ring',\n       'ground_ele_median_150m_ring', 'ground_ele_median_100m',\n       'building_area_std_1000m_kml', 'building_area_min_1000m_kml_ring',\n       'building_area_median_1000m_kml_ring',\n       'building_area_std_1000m_kml_ring', 'building_area_min_500m_kml',\n       'building_area_median_500m_kml', 'building_area_std_500m_kml',\n       'building_area_median_500m_kml_ring', 'building_area_min_400m_kml',\n       'building_area_std_400m_kml', 'building_area_min_400m_kml_ring',\n       'building_area_std_300m_kml', 'building_area_min_250m_kml',\n       'building_area_std_250m_kml_ring']\n\ncombined_nulls = list(set(list(set(train_nulls)) + list(set(test_nulls))))\nprint(len(combined_nulls))","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:02.996414Z","iopub.execute_input":"2025-03-23T15:17:02.996821Z","iopub.status.idle":"2025-03-23T15:17:03.004738Z","shell.execute_reply.started":"2025-03-23T15:17:02.996782Z","shell.execute_reply":"2025-03-23T15:17:03.003573Z"},"papermill":{"duration":0.025914,"end_time":"2025-03-20T10:34:57.988736","exception":false,"start_time":"2025-03-20T10:34:57.962822","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# null_counts_test = test_peak.isnull().sum()\n# null_counts[null_counts>0].index","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.006600Z","iopub.execute_input":"2025-03-23T15:17:03.006898Z","iopub.status.idle":"2025-03-23T15:17:03.023974Z","shell.execute_reply.started":"2025-03-23T15:17:03.006870Z","shell.execute_reply":"2025-03-23T15:17:03.022901Z"},"papermill":{"duration":0.02164,"end_time":"2025-03-20T10:34:58.024946","exception":false,"start_time":"2025-03-20T10:34:58.003306","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# null_counts = train_peak.isnull().sum()\n# null_counts[null_counts>0].index","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.025279Z","iopub.execute_input":"2025-03-23T15:17:03.026293Z","iopub.status.idle":"2025-03-23T15:17:03.040887Z","shell.execute_reply.started":"2025-03-23T15:17:03.026255Z","shell.execute_reply":"2025-03-23T15:17:03.039575Z"},"papermill":{"duration":0.021576,"end_time":"2025-03-20T10:34:58.061124","exception":false,"start_time":"2025-03-20T10:34:58.039548","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_peak.drop(columns=combined_nulls, inplace=True)\n# test_peak.drop(columns=combined_nulls, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.042038Z","iopub.execute_input":"2025-03-23T15:17:03.042396Z","iopub.status.idle":"2025-03-23T15:17:03.059592Z","shell.execute_reply.started":"2025-03-23T15:17:03.042353Z","shell.execute_reply":"2025-03-23T15:17:03.058014Z"},"papermill":{"duration":0.035312,"end_time":"2025-03-20T10:34:58.110922","exception":false,"start_time":"2025-03-20T10:34:58.075610","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feats_1 = ['lwir11_lst',\n 'EVI',\n 'Albedo',\n 'building_area_4000m',\n 'building_count_4000m',\n 'building_height_mean_4000m',\n 'ground_ele_mean_4000m',\n 'building_count_3500m',\n 'ground_ele_mean_3500m',\n 'building_area_3000m',\n 'building_count_3000m',\n 'building_height_max_3000m',\n 'ground_ele_mean_3000m',\n 'building_area_2500m',\n 'building_count_2500m',\n 'ground_ele_mean_2500m',\n 'building_area_2000m',\n 'building_count_2000m',\n 'ground_ele_mean_2000m',\n 'building_area_1500m',\n 'building_height_max_1500m',\n 'ground_ele_mean_1500m',\n 'building_area_1000m',\n 'building_height_max_1000m',\n 'ground_ele_mean_1000m',\n 'building_count_500m',\n 'building_height_max_500m',\n 'ground_ele_mean_500m',\n 'building_area_15000m_kml',\n 'building_area_11500m_kml',\n 'building_area_11000m_kml',\n 'building_area_10500m_kml',\n 'building_count_8000m_kml',\n 'building_count_7000m_kml',\n 'building_count_6500m_kml',\n 'building_area_1500m_kml',\n 'building_count_1500m_kml',\n 'building_count_1000m_kml',\n 'building_area_250m_kml',\n 'Weather_AirTemp',\n 'Weather_RH',\n 'Weather_WindSpd',\n 'Weather_WindDir',\n 'Weather_Solar',\n 'distance_to_water',\n 'distance_to_park',\n 'node_density_osm',\n 'elev_count_500m',\n 'mean_elev_500m',\n 'max_elev_500m',\n 'min_elev_500m',\n 'elev_count_1000m',\n 'mean_elev_1000m',\n 'max_elev_1000m',\n 'min_elev_1000m',\n 'std_elev_1000m',\n 'elev_range_1000m',\n 'elev_count_1500m',\n 'max_elev_1500m',\n 'min_elev_1500m',\n 'std_elev_1500m',\n 'elev_range_1500m',\n 'slope_est_1500m',\n 'elev_count_2000m',\n 'mean_elev_2000m',\n 'min_elev_2000m',\n 'std_elev_2000m',\n 'elev_count_2500m',\n 'mean_elev_2500m',\n 'min_elev_2500m',\n 'std_elev_2500m',\n 'elev_count_3000m',\n 'mean_elev_3000m',\n 'std_elev_3000m',\n 'elev_count_3500m',\n 'std_elev_3500m',\n 'std_elev_4000m',\n 'elev_count_4500m',\n 'min_elev_4500m',\n 'std_elev_4500m',\n 'mean_elev_5000m',\n 'mean_elev_5500m',\n 'mean_elev_8500m',\n 'elev_count_9000m',\n 'std_elev_9000m',\n 'elev_count_10000m',\n 'elev_count_11000m',\n 'min_elev_11000m',\n 'elev_range_11000m',\n 'slope_est_11000m',\n 'elev_count_11500m',\n 'elev_range_11500m',\n 'mean_elev_12000m',\n 'max_elev_12000m',\n 'elev_range_12000m',\n 'slope_est_12000m',\n 'max_elev_12500m',\n 'elev_range_12500m',\n 'slope_est_12500m',\n 'max_elev_13000m',\n 'elev_range_13000m',\n 'slope_est_13000m',\n 'mean_elev_13500m',\n 'slope_est_13500m',\n 'mean_elev_14000m',\n 'max_elev_14000m',\n 'elev_range_14000m',\n 'slope_est_14000m',\n 'tree_count_500m',\n 'avg_tree_health_500m',\n 'max_tree_size_500m',\n 'tree_count_1000m',\n 'avg_tree_health_1000m',\n 'tree_count_1500m',\n 'avg_tree_health_1500m',\n 'max_tree_size_1500m',\n 'tree_count_2000m',\n 'avg_tree_health_2000m',\n 'tree_count_2500m',\n 'avg_tree_health_2500m',\n 'tree_count_3000m',\n 'avg_tree_health_3000m',\n 'tree_count_4000m',\n 'avg_tree_health_4000m',\n 'max_tree_size_4000m',\n 'avg_tree_health_4500m',\n 'max_tree_size_4500m',\n 'avg_tree_health_5000m',\n 'avg_tree_health_5500m',\n 'max_tree_size_5500m',\n 'tree_count_6000m',\n 'tree_count_6500m',\n 'avg_tree_health_6500m',\n 'max_tree_size_6500m',\n 'tree_count_7000m',\n 'avg_tree_health_9000m',\n 'avg_tree_health_12500m',\n 'avg_tree_health_13000m',\n 'max_tree_size_13500m',\n 'max_tree_size_14000m',\n 'mean_dbh_500m',\n 'max_dbh_500m',\n 'mean_risk_500m',\n 'max_stump_500m',\n 'forestry_count_500m',\n 'mean_dbh_1000m',\n 'max_dbh_1000m',\n 'mean_risk_1000m',\n 'max_stump_1000m',\n 'forestry_count_1000m',\n 'mean_dbh_1500m',\n 'max_dbh_1500m',\n 'mean_risk_1500m',\n 'max_stump_1500m',\n 'forestry_count_1500m',\n 'max_dbh_2000m',\n 'mean_risk_2000m',\n 'max_stump_2000m',\n 'forestry_count_2000m',\n 'max_dbh_2500m',\n 'max_stump_2500m',\n 'forestry_count_2500m',\n 'max_dbh_3000m',\n 'forestry_count_3000m',\n 'mean_dbh_3500m',\n 'max_dbh_3500m',\n 'forestry_count_3500m',\n 'mean_dbh_4000m',\n 'max_dbh_4000m',\n 'mean_risk_4000m',\n 'forestry_count_4000m',\n 'max_dbh_4500m',\n 'forestry_count_4500m',\n 'max_dbh_5000m',\n 'forestry_count_5000m',\n 'max_dbh_5500m',\n 'mean_risk_5500m',\n 'forestry_count_5500m',\n 'mean_risk_6000m',\n 'mean_risk_6500m',\n 'max_stump_6500m',\n 'mean_risk_7000m',\n 'mean_risk_7500m',\n 'max_dbh_8000m',\n 'mean_risk_8000m',\n 'max_dbh_8500m',\n 'mean_risk_8500m',\n 'mean_risk_9000m',\n 'mean_risk_10000m',\n 'forestry_count_10000m',\n 'mean_dbh_11500m',\n 'mean_risk_12500m',\n 'mean_dbh_13000m',\n 'max_dbh_13000m',\n 'mean_risk_13000m',\n 'mean_risk_13500m',\n 'mean_risk_14000m',\n 'traffic_count_1000m',\n 'traffic_vol_1500m',\n 'traffic_count_1500m',\n 'traffic_count_2000m',\n 'traffic_vol_2500m',\n 'traffic_count_2500m',\n 'traffic_count_3000m',\n 'traffic_vol_4500m',\n 'traffic_count_4500m',\n 'traffic_vol_5000m',\n 'traffic_vol_5500m',\n 'traffic_vol_6000m',\n 'traffic_vol_7000m',\n 'traffic_count_7000m',\n 'traffic_vol_7500m',\n 'traffic_vol_8000m',\n 'traffic_vol_8500m',\n 'traffic_vol_9000m',\n 'traffic_vol_11000m',\n 'traffic_count_11000m',\n 'traffic_count_13000m',\n 'traffic_count_13500m',\n 'traffic_vol_14000m',\n 'traffic_count_14000m']\nfeats_2 = [feature for feature in feats_1 if feature!='Albedo']\ntest_com[feats_1].shape","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.060770Z","iopub.execute_input":"2025-03-23T15:17:03.061179Z","iopub.status.idle":"2025-03-23T15:17:03.086531Z","shell.execute_reply.started":"2025-03-23T15:17:03.061136Z","shell.execute_reply":"2025-03-23T15:17:03.085597Z"},"papermill":{"duration":0.057922,"end_time":"2025-03-20T10:34:58.183461","exception":false,"start_time":"2025-03-20T10:34:58.125539","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_com[feats_2].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T15:17:03.087803Z","iopub.execute_input":"2025-03-23T15:17:03.088205Z","iopub.status.idle":"2025-03-23T15:17:03.112176Z","shell.execute_reply.started":"2025-03-23T15:17:03.088154Z","shell.execute_reply":"2025-03-23T15:17:03.111043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_peak_imp_1 = train_com[feats_1].fillna(0)\ntest_peak_imp_1 = test_com[feats_1].fillna(0)\n\ntrain_peak_imp = train_com[feats_2].fillna(0)\ntest_peak_imp = test_com[feats_2].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.114783Z","iopub.execute_input":"2025-03-23T15:17:03.115152Z","iopub.status.idle":"2025-03-23T15:17:03.166340Z","shell.execute_reply.started":"2025-03-23T15:17:03.115112Z","shell.execute_reply":"2025-03-23T15:17:03.165194Z"},"papermill":{"duration":0.043412,"end_time":"2025-03-20T10:34:58.241862","exception":false,"start_time":"2025-03-20T10:34:58.198450","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Out-of-Fold (OOF) Predictions\n\nWe generate OOF predictions using multiple variants of the ExtraTreesRegressor. This strategy promotes model diversity and robust validation:\n- **Scaled Features:** A subset of models (e.g., `etr_25`, `etr_27`, `etr_28`) use scaled features.\n- **Unscaled Features:** The remaining models use unscaled features (with an extra Albedo feature) to increase randomness.\n- **Validation:** Rigorous cross-validation is applied to ensure that the OOF predictions are reliable and avoid overfitting.\n\n---","metadata":{}},{"cell_type":"code","source":"seed=1","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.168404Z","iopub.execute_input":"2025-03-23T15:17:03.168853Z","iopub.status.idle":"2025-03-23T15:17:03.173496Z","shell.execute_reply.started":"2025-03-23T15:17:03.168810Z","shell.execute_reply":"2025-03-23T15:17:03.172178Z"},"papermill":{"duration":0.023413,"end_time":"2025-03-20T10:34:58.431933","exception":false,"start_time":"2025-03-20T10:34:58.408520","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef generate_oof_preds(unscaled_df, scaled_df, target, model_dict, n_splits=5, seed=1):\n    \"\"\"\n    Generates out-of-fold predictions for a set of models.\n    \n    For models whose names are in ['etr_25', 'etr_27', 'etr_28']:\n      - The unscaled data is used and StandardScaler is applied _inside each fold_,\n        ensuring that each fold’s scaler is fit solely on its training data.\n      \n    For all other models:\n      - The unscaled data is used directly.\n    \n    Parameters:\n      unscaled_df (pd.DataFrame): Raw training features.\n      scaled_df (pd.DataFrame): Pre-scaled training features (unused here for the specified models).\n      target (pd.Series): Target variable.\n      model_dict (dict): Dictionary of models to train.\n      n_splits (int, optional): Number of CV folds.\n      seed (int, optional): Random seed for shuffling.\n      \n    Returns:\n      pd.DataFrame: Out-of-fold predictions for each model.\n    \"\"\"\n    \n    unscaled_df = unscaled_df.reset_index(drop=True)\n    scaled_df = scaled_df.reset_index(drop=True)\n    target = target.reset_index(drop=True)\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof_preds = pd.DataFrame(index=unscaled_df.index)\n    \n    for name, model in model_dict.items():\n        print(f\"Processing model: {name}\")\n        oof_model_preds = np.zeros(len(unscaled_df))\n        fold_scores = []\n        \n        for fold, (train_idx, valid_idx) in enumerate(kf.split(unscaled_df, target)):\n            if name in ['etr_25', 'etr_27', 'etr_28']:\n                X_train = scaled_df.iloc[train_idx].copy()\n                X_valid = scaled_df.iloc[valid_idx].copy()\n                \n                scaler = StandardScaler()\n                X_train_used = scaler.fit_transform(X_train)\n                X_valid_used = scaler.transform(X_valid)\n            else:\n                X_train = unscaled_df.iloc[train_idx]\n                X_valid = unscaled_df.iloc[valid_idx]\n                X_train_used = X_train\n                X_valid_used = X_valid\n            \n            y_train = target.iloc[train_idx]\n            y_valid = target.iloc[valid_idx]\n            \n            model_clone = clone(model)\n            model_clone.fit(X_train_used, y_train)\n            preds = model_clone.predict(X_valid_used)\n            \n            oof_model_preds[valid_idx] = preds\n            fold_score = r2_score(y_valid, preds)\n            fold_scores.append(fold_score)\n            print(f\"  Fold {fold + 1} R²: {fold_score:.4f}\")\n        \n            del X_train, X_valid, model_clone, preds\n            gc.collect()\n        \n        print(f\"Mean R² for {name}: {np.mean(fold_scores):.4f}\\n\")\n        oof_preds[name] = oof_model_preds\n        \n    return oof_preds.sort_index()\n","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.174710Z","iopub.execute_input":"2025-03-23T15:17:03.175014Z","iopub.status.idle":"2025-03-23T15:17:03.195559Z","shell.execute_reply.started":"2025-03-23T15:17:03.174982Z","shell.execute_reply":"2025-03-23T15:17:03.194422Z"},"papermill":{"duration":1.588909,"end_time":"2025-03-20T10:35:00.035982","exception":false,"start_time":"2025-03-20T10:34:58.447073","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_test_preds(unscaled_train, scaled_train, target, unscaled_test, scaled_test, model_dict, save_dir=\"/kaggle/working/models/\", seed=1):\n    \"\"\"\n    Generates test predictions using scaled data for etr_25/28/29, saves trained models,\n    and returns test predictions.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    test_preds = pd.DataFrame(index=unscaled_test.index)\n    \n    for name, model in model_dict.items():\n        # Select appropriate datasets\n        train_data = scaled_train if name in ['etr_25', 'etr_27', 'etr_28'] else unscaled_train\n        test_data = scaled_test if name in ['etr_25', 'etr_27', 'etr_28'] else unscaled_test\n        \n        model_clone = clone(model)\n        model_clone.fit(train_data, target)\n        \n        # Save model with versioning\n        model_path = os.path.join(save_dir, f\"{name}.pkl\")\n        joblib.dump(model_clone, model_path)\n        print(f\"Saved {name} to {model_path}\")\n        \n        test_preds[name] = model_clone.predict(test_data)\n    \n    return test_preds","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.196770Z","iopub.execute_input":"2025-03-23T15:17:03.197063Z","iopub.status.idle":"2025-03-23T15:17:03.217745Z","shell.execute_reply.started":"2025-03-23T15:17:03.197037Z","shell.execute_reply":"2025-03-23T15:17:03.216798Z"},"papermill":{"duration":0.023449,"end_time":"2025-03-20T10:35:00.074662","exception":false,"start_time":"2025-03-20T10:35:00.051213","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\noof_scaler = StandardScaler()\ntrain_peak_imp_scaled = oof_scaler.fit_transform(train_peak_imp_1)\ntest_peak_imp_scaled = oof_scaler.transform(test_peak_imp_1)\n\n\ntrain_peak_imp_scaled_df = pd.DataFrame(train_peak_imp_scaled, columns = train_peak_imp_1.columns)\ntest_peak_imp_scaled_df = pd.DataFrame(test_peak_imp_scaled, columns = test_peak_imp_1.columns)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.218674Z","iopub.execute_input":"2025-03-23T15:17:03.218971Z","iopub.status.idle":"2025-03-23T15:17:03.309254Z","shell.execute_reply.started":"2025-03-23T15:17:03.218919Z","shell.execute_reply":"2025-03-23T15:17:03.308279Z"},"papermill":{"duration":0.105355,"end_time":"2025-03-20T10:35:00.194932","exception":false,"start_time":"2025-03-20T10:35:00.089577","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_peak_imp_scaled_df.shape)\nprint(test_peak_imp_scaled_df.shape)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.310256Z","iopub.execute_input":"2025-03-23T15:17:03.310661Z","iopub.status.idle":"2025-03-23T15:17:03.317112Z","shell.execute_reply.started":"2025-03-23T15:17:03.310609Z","shell.execute_reply":"2025-03-23T15:17:03.315710Z"},"papermill":{"duration":0.023135,"end_time":"2025-03-20T10:35:00.233088","exception":false,"start_time":"2025-03-20T10:35:00.209953","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_dict = {\n    # 5 variants for ExtraTreesRegressor (ETR)\n    'etr_o': ExtraTreesRegressor(n_estimators=200, max_depth=55, random_state=seed, n_jobs=-1),\n    'etr_1': ExtraTreesRegressor(n_estimators=200, max_depth=55, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.8, bootstrap=False, \n                                 criterion='squared_error', random_state=seed, n_jobs=-1),\n    'etr_2': ExtraTreesRegressor(n_estimators=250, max_depth=60, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.15, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+1, n_jobs=-1),\n    'etr_3': ExtraTreesRegressor(n_estimators=150, max_depth=50, min_samples_split=3, \n                                 min_samples_leaf=1, max_features=0.3, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+2, n_jobs=-1),\n    'etr_4': ExtraTreesRegressor(n_estimators=300, max_depth=55, min_samples_split=3, \n                                 min_samples_leaf=2, max_features=0.55, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+3, n_jobs=-1),\n    'etr_5': ExtraTreesRegressor(n_estimators=200, max_depth=60, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.88, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+4, n_jobs=-1),\n     # Additional diverse variants:\n    # Variant with friedman_mse\n    'etr_6': ExtraTreesRegressor(n_estimators=220, max_depth=None, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.45, bootstrap=False, \n                                 criterion='friedman_mse', random_state=seed+5, n_jobs=-1),\n    # # # # Variant with no max_depth limit\n    'etr_7': ExtraTreesRegressor(n_estimators=300, max_depth=None, min_samples_split=4, \n                                 min_samples_leaf=2, max_features=0.3, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+6, n_jobs=-1),\n    # Variant with shallower trees but more estimators\n    'etr_8': ExtraTreesRegressor(n_estimators=400, max_depth=40, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.4, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+7, n_jobs=-1),\n    # Variant with lower max_features\n    'etr_9': ExtraTreesRegressor(n_estimators=200, max_depth=60, min_samples_split=2, \n                                 min_samples_leaf=1, max_features=0.2, bootstrap=False, \n                                 criterion='squared_error', random_state=seed+8, n_jobs=-1),\n    'etr_10': ExtraTreesRegressor(\n        n_estimators=220, max_depth=55, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.3, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+10, n_jobs=-1\n    ),\n    'etr_11': ExtraTreesRegressor(\n        n_estimators=250, max_depth=60, min_samples_split=3, min_samples_leaf=1,\n        max_features=0.35, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+11, n_jobs=-1\n    ),\n    'etr_12': ExtraTreesRegressor(\n        n_estimators=200, max_depth=55, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.3, bootstrap=False, criterion='poisson',\n        random_state=seed+12, n_jobs=-1\n    ),\n    'etr_13': ExtraTreesRegressor(\n        n_estimators=250, max_depth=60, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.35, bootstrap=False, criterion='poisson',\n        random_state=seed+13, n_jobs=-1\n    ),\n    'etr_14': ExtraTreesRegressor(\n        n_estimators=300, max_depth=50, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.3, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+14, n_jobs=-1\n    ),\n    'etr_15': ExtraTreesRegressor(\n        n_estimators=300, max_depth=60, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.25, bootstrap=False, criterion='poisson',\n        random_state=seed+15, n_jobs=-1\n    ),\n    'etr_16': ExtraTreesRegressor(\n        n_estimators=200, max_depth=None, min_samples_split=3, min_samples_leaf=1,\n        max_features=0.3, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+16, n_jobs=-1\n    ),\n    'etr_17': ExtraTreesRegressor(\n        n_estimators=200, max_depth=None, min_samples_split=3, min_samples_leaf=1,\n        max_features=0.3, bootstrap=False, criterion='poisson',\n        random_state=seed+17, n_jobs=-1\n    ),\n    'etr_18': ExtraTreesRegressor(\n        n_estimators=350, max_depth=40, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.4, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+18, n_jobs=-1\n    ),\n    'etr_19': ExtraTreesRegressor(\n        n_estimators=350, max_depth=40, min_samples_split=2, min_samples_leaf=1,\n        max_features=0.4, bootstrap=False, criterion='poisson',\n        random_state=seed+19, n_jobs=-1\n    ),\n    'etr_20': ExtraTreesRegressor(\n        n_estimators=250, max_depth=55, min_samples_split=2, min_samples_leaf=2,\n        max_features=0.3, bootstrap=False, criterion='friedman_mse',\n        random_state=seed+20, n_jobs=-1\n    ),\n    'etr_24': ExtraTreesRegressor(n_estimators=50, max_depth=25, min_samples_split=4, \n                              min_samples_leaf=2, max_features=0.3, bootstrap=False, \n                              criterion='squared_error', random_state=seed+24, n_jobs=-1),\n    'etr_23': ExtraTreesRegressor(n_estimators=300, max_depth=55, min_samples_split=8, \n                              min_samples_leaf=3, max_features=0.35, bootstrap=False, \n                              criterion='squared_error', random_state=seed+23, n_jobs=-1),\n    'etr_22': ExtraTreesRegressor(n_estimators=250, max_depth=None, min_samples_split=2, \n                              min_samples_leaf=1, max_features=0.3, bootstrap=True, \n                              criterion='squared_error', random_state=seed+22, n_jobs=-1),\n    'etr_25': ExtraTreesRegressor(\n        n_estimators=500, \n        max_depth=None,  # Fully grow trees\n        min_samples_split=2, \n        min_samples_leaf=1,\n        max_features=0.1,  # Very low for diversity\n        criterion='friedman_mse',  # Better for gradients\n        bootstrap=False,\n        random_state=seed+25,\n        n_jobs=-1\n    ),\n\n    'etr_27': ExtraTreesRegressor(\n        n_estimators=300,\n        max_depth=80,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features=0.05,  # Forces diverse splits\n        criterion='squared_error',\n        bootstrap=False,\n        random_state=seed+27,\n        n_jobs=-1\n    ),\n    # Poisson criterion for count-like targets\n    'etr_28': ExtraTreesRegressor(\n        n_estimators=350,\n        max_depth=60,\n        min_samples_split=2,\n        min_samples_leaf=1,\n        max_features=0.15,\n        criterion='poisson',  # For exponential relationships\n        bootstrap=False,\n        random_state=seed+28,\n        n_jobs=-1\n    ),\n\n}\noof_preds = generate_oof_preds(train_peak_imp, train_peak_imp_1, target, model_dict, n_splits=10, seed=1)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:17:03.318664Z","iopub.execute_input":"2025-03-23T15:17:03.318960Z","iopub.status.idle":"2025-03-23T15:25:01.634515Z","shell.execute_reply.started":"2025-03-23T15:17:03.318934Z","shell.execute_reply":"2025-03-23T15:25:01.633346Z"},"papermill":{"duration":3479.401228,"end_time":"2025-03-20T11:32:59.649398","exception":false,"start_time":"2025-03-20T10:35:00.248170","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds = generate_test_preds(train_peak_imp, train_peak_imp_scaled_df, target, test_peak_imp, test_peak_imp_scaled_df, model_dict, seed=1)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:27:26.817231Z","iopub.execute_input":"2025-03-23T15:27:26.818088Z","iopub.status.idle":"2025-03-23T15:28:23.735575Z","shell.execute_reply.started":"2025-03-23T15:27:26.818038Z","shell.execute_reply":"2025-03-23T15:28:23.734217Z"},"papermill":{"duration":376.943542,"end_time":"2025-03-20T11:39:16.624307","exception":false,"start_time":"2025-03-20T11:32:59.680765","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds.head(3)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:25:56.077526Z","iopub.execute_input":"2025-03-23T15:25:56.077919Z","iopub.status.idle":"2025-03-23T15:25:56.101127Z","shell.execute_reply.started":"2025-03-23T15:25:56.077881Z","shell.execute_reply":"2025-03-23T15:25:56.100086Z"},"papermill":{"duration":0.061244,"end_time":"2025-03-20T11:39:16.719226","exception":false,"start_time":"2025-03-20T11:39:16.657982","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\noof_preds.head(3)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:25:56.102140Z","iopub.execute_input":"2025-03-23T15:25:56.102520Z","iopub.status.idle":"2025-03-23T15:25:56.113427Z","shell.execute_reply.started":"2025-03-23T15:25:56.102457Z","shell.execute_reply":"2025-03-23T15:25:56.112452Z"},"papermill":{"duration":0.060233,"end_time":"2025-03-20T11:39:16.813776","exception":false,"start_time":"2025-03-20T11:39:16.753543","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Alternate Optimization Techniques\n\nFor weighted average ensembling, two optimization approaches:\n- **Hill Climbing:** An iterative process to adjust ensemble weights.\n- **Genetic Algorithm:** A stochastic method that mimics natural evolution to optimize weight allocation.\n\n---","metadata":{"papermill":{"duration":0.032787,"end_time":"2025-03-20T11:39:16.879097","exception":false,"start_time":"2025-03-20T11:39:16.846310","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"###  5.1 HILL CLIMIBING","metadata":{"papermill":{"duration":0.032073,"end_time":"2025-03-20T11:39:17.100572","exception":false,"start_time":"2025-03-20T11:39:17.068499","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n\ndef evaluate_combination(weights, preds, y_true):\n    ensemble_pred = np.average(preds, axis=1, weights=weights)\n    return r2_score(y_true, ensemble_pred)\n\ndef hill_climbing(preds, y_true, max_iter=100000, patience=900):\n    n_models = preds.shape[1]\n    best_weights = np.ones(n_models) / n_models # same weight for all features initially\n    best_score = evaluate_combination(best_weights, preds, y_true)\n    n_iter = 0\n    local_iter = 0\n    for i in range(max_iter):\n        n_iter=i\n        candidate_weights = best_weights + np.random.normal(0, 0.00001, best_weights.shape) # adding random noise with normal dist\n        # candidate_weights = np.clip(candidate_weights, 0, 1) # zeroing negative values\n        candidate_weights = candidate_weights / candidate_weights.sum() # normalizing\n        candidate_score   = evaluate_combination(candidate_weights, preds, y_true)\n        \n        if candidate_score > best_score:  \n            best_score = candidate_score\n            best_weights = candidate_weights\n            local_iter = 0\n        else:\n            local_iter+=1\n            \n        if local_iter>=patience:\n            break\n            \n    return best_score, best_weights, n_iter\n\n\n# best_score, best_weights, n_iter = hill_climbing(oof_preds, target, max_iter=100000, patience=90000)\n# print(f'best score with hill climbing at iter {n_iter}   :', best_score)\n# print(f'best weights with hill climbing at iter {n_iter} :', best_weights)\n\n# preds = np.average(test_preds, weights=best_weights, axis=1)\n# preds","metadata":{"execution":{"iopub.status.busy":"2025-03-23T16:00:48.805646Z","iopub.execute_input":"2025-03-23T16:00:48.806004Z","iopub.status.idle":"2025-03-23T16:01:59.565201Z","shell.execute_reply.started":"2025-03-23T16:00:48.805977Z","shell.execute_reply":"2025-03-23T16:01:59.563924Z"},"papermill":{"duration":0.042778,"end_time":"2025-03-20T11:39:17.175554","exception":false,"start_time":"2025-03-20T11:39:17.132776","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Genetic Algorithm","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import r2_score\n\ndef evaluate_combination(weights, preds, y_true):\n    ensemble_pred = np.average(preds, axis=1, weights=weights)\n    return r2_score(y_true, ensemble_pred)\n\ndef initialize_population(n_models, population_size):\n    population = np.random.rand(population_size, n_models)\n    population /= population.sum(axis=1, keepdims=True)  # Normalize\n    return population\n\ndef select_parents(population, fitness, num_parents):\n    parents = population[np.argsort(fitness)[-num_parents:]]  # Select top candidates\n    return parents\n\ndef crossover(parents, offspring_size):\n    offspring = np.empty(offspring_size)\n    for k in range(offspring_size[0]):\n        # Randomly select two parents\n        parent1, parent2 = parents[np.random.choice(len(parents), 2, replace=False)]\n        # Blend weights\n        alpha = np.random.rand()\n        offspring[k] = alpha * parent1 + (1 - alpha) * parent2\n    return offspring\n\ndef mutate(offspring, mutation_rate=0.1):\n    for candidate in offspring:\n        if np.random.rand() < mutation_rate:\n            # Add small random noise\n            candidate += np.random.normal(0, 0.01, size=candidate.shape)\n            candidate = np.clip(candidate, 0, None)  # Ensure non-negative weights\n            candidate /= candidate.sum()  # Normalize\n    return offspring\n\ndef genetic_algorithm(preds, y_true, n_models, population_size=50, num_generations=100, mutation_rate=0.1):\n    population = initialize_population(n_models, population_size)\n    best_score = -np.inf\n    best_weights = None\n\n    for generation in range(num_generations):\n        # Evaluate fitness\n        fitness = np.array([evaluate_combination(weights, preds, y_true) for weights in population])\n        \n        # Track best solution\n        if np.max(fitness) > best_score:\n            best_score = np.max(fitness)\n            best_weights = population[np.argmax(fitness)]\n        \n        # Select parents\n        parents = select_parents(population, fitness, num_parents=population_size // 2)\n        \n        # Generate offspring\n        offspring = crossover(parents, offspring_size=(population_size - len(parents), n_models))\n        \n        # Mutate offspring\n        offspring = mutate(offspring, mutation_rate)\n        \n        # Create new population\n        population = np.vstack([parents, offspring])\n    \n    return best_score, best_weights\n\n# best_score, best_weights = genetic_algorithm(oof_preds, target, n_models=oof_preds.shape[1])\n# print(f'Best score with GA: {best_score:.4f}')\n# print(f'Best weights with GA: {best_weights}')\n\n# preds = np.average(test_preds, weights=best_weights, axis=1)\n# preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T15:25:56.134007Z","iopub.execute_input":"2025-03-23T15:25:56.134377Z","iopub.status.idle":"2025-03-23T15:25:56.149916Z","shell.execute_reply.started":"2025-03-23T15:25:56.134341Z","shell.execute_reply":"2025-03-23T15:25:56.148832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Meta-Model & Final Inference\n\nFinal predictions are produced by blending the OOF predictions through a Ridge Kernel meta-model:\n- **Inference:** The meta-model is trained on the OOF predictions and then applied to generate test predictions.\n- **Submission:** The test predictions are formatted and saved for final submission.\n\n---","metadata":{}},{"cell_type":"code","source":"oof_preds.to_csv('etr_21_oof_preds_10f_0.9887.csv', index=False)\ntest_preds.to_csv('etr_21_test_preds_10f_0.9837.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:25:56.150975Z","iopub.execute_input":"2025-03-23T15:25:56.151261Z","iopub.status.idle":"2025-03-23T15:25:56.268141Z","shell.execute_reply.started":"2025-03-23T15:25:56.151236Z","shell.execute_reply":"2025-03-23T15:25:56.267054Z"},"papermill":{"duration":0.601193,"end_time":"2025-03-20T11:39:18.027769","exception":false,"start_time":"2025-03-20T11:39:17.426576","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler_2 = StandardScaler()\noof_preds_scaled = scaler_2.fit_transform(oof_preds)\ntest_preds_scaled = scaler_2.transform(test_preds)\n# lr = Ridge(alpha=0.0001, solver='lsqr')\nlr = KernelRidge(\n    alpha=0.00001, \n    kernel='poly',  # Mild non-linearity\n    degree=2,  # Quadratic interactions\n    # gamma=0.01,\n    coef0=0.25,\n)\n# lr = HuberRegressor(epsilon=1.5, alpha=0.0001, max_iter=5000)\n# lr = ElasticNet(\n#     alpha=0.0001,  # Match Ridge's regularization strength\n#     l1_ratio=0.1,  # 10% L1 (sparsity), 90% L2 (small coefficients)\n#     max_iter=5000,\n#     random_state=42\n# )\n# lr = SVR()\n# lr = xgb.XGBRegressor(\n#     objective='reg:squarederror',\n#     booster='gbtree',\n#     n_estimators=350,\n#     learning_rate=0.03,\n#     max_depth=9,\n#     # min_child_weight=2,\n#     # subsample=0.8,\n#     # colsample_bytree=0.9,\n#     # gamma=0.1,\n#     random_state=42,\n#     n_jobs=-1\n#     )\n# lr = HistGradientBoostingRegressor(random_state=1)\n# lr = lgb.LGBMRegressor(random_state=42, verbosity=0)\n# lr = ExtraTreesRegressor(n_jobs=-1, n_estimators=200, max_depth=55, random_state=42)\n# lr = KNeighborsRegressor(n_neighbors=4)\n# lr = Lasso(alpha=1e-07, max_iter=15000, random_state=1)\n# lr = LinearRegression()\n# target_bc, Iam = boxcox(target)\n# target_log = np.log(target)\n# np.random.seed(42)\n# noise = np.random.normal(0, 0.1, oof_preds_scaled.shape)\n# oof_preds_noisy = oof_preds_scaled + noise\nlr.fit(oof_preds_scaled, target)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:25:56.269174Z","iopub.execute_input":"2025-03-23T15:25:56.269451Z","iopub.status.idle":"2025-03-23T15:26:21.544205Z","shell.execute_reply.started":"2025-03-23T15:25:56.269418Z","shell.execute_reply":"2025-03-23T15:26:21.543082Z"},"papermill":{"duration":30.657885,"end_time":"2025-03-20T11:39:49.091539","exception":false,"start_time":"2025-03-20T11:39:18.433654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.1 Inference and Submission","metadata":{}},{"cell_type":"code","source":"preds = lr.predict(test_preds_scaled)\nprint(preds)\ntest_df['UHI Index'] = preds\ntest_df.to_csv('submission_peak_hc.csv', index=False)\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2025-03-23T15:26:21.556138Z","iopub.execute_input":"2025-03-23T15:26:21.556438Z","iopub.status.idle":"2025-03-23T15:26:21.633484Z","shell.execute_reply.started":"2025-03-23T15:26:21.556413Z","shell.execute_reply":"2025-03-23T15:26:21.632560Z"},"papermill":{"duration":0.054083,"end_time":"2025-03-20T11:39:49.333844","exception":false,"start_time":"2025-03-20T11:39:49.279761","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}