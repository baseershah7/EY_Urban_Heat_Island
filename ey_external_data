{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10580423,"sourceType":"datasetVersion","datasetId":6547867},{"sourceId":10580457,"sourceType":"datasetVersion","datasetId":6547892},{"sourceId":10752312,"sourceType":"datasetVersion","datasetId":6668789},{"sourceId":10798353,"sourceType":"datasetVersion","datasetId":6701956}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. UHI Prediction --- external data \nfollowing datasets are extracted in this notebook:\n- ELEVATION data\n- Stree Tree Cencus\n- Forestry data\n- Traffic data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:30:48.150389Z","iopub.execute_input":"2025-03-18T19:30:48.150732Z","iopub.status.idle":"2025-03-18T19:30:48.663854Z","shell.execute_reply.started":"2025-03-18T19:30:48.150703Z","shell.execute_reply":"2025-03-18T19:30:48.662695Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. ELEVATION ","metadata":{}},{"cell_type":"code","source":"nyc_elevation = pd.read_csv('/kaggle/input/nyc-elevation-data/NYC_Planimetric_Database__Elevation_Points_20250213.csv')\nnyc_elevation.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:30:48.665174Z","iopub.execute_input":"2025-03-18T19:30:48.665825Z","iopub.status.idle":"2025-03-18T19:30:53.221440Z","shell.execute_reply.started":"2025-03-18T19:30:48.665752Z","shell.execute_reply":"2025-03-18T19:30:53.220544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/uhi-index-data/Training_data_uhi_index_UHI2025-v2.csv')\ntest  = pd.read_csv('/kaggle/input/test-df-ey-open-science/Submission_template_UHI2025-v2.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:30:53.222464Z","iopub.execute_input":"2025-03-18T19:30:53.222796Z","iopub.status.idle":"2025-03-18T19:30:53.252498Z","shell.execute_reply.started":"2025-03-18T19:30:53.222748Z","shell.execute_reply":"2025-03-18T19:30:53.251393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:30:53.254659Z","iopub.execute_input":"2025-03-18T19:30:53.255042Z","iopub.status.idle":"2025-03-18T19:30:53.284724Z","shell.execute_reply.started":"2025-03-18T19:30:53.255010Z","shell.execute_reply":"2025-03-18T19:30:53.283633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import geopandas as gpd\nfrom shapely import wkt\n\nnyc_elevation['geometry'] = nyc_elevation['the_geom'].apply(wkt.loads)\nnyc_elevation = gpd.GeoDataFrame(nyc_elevation, geometry='geometry', crs='EPSG:4326')\nnyc_elevation = nyc_elevation.to_crs('EPSG:2263')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:30:53.286431Z","iopub.execute_input":"2025-03-18T19:30:53.286814Z","iopub.status.idle":"2025-03-18T19:31:11.001293Z","shell.execute_reply.started":"2025-03-18T19:30:53.286740Z","shell.execute_reply":"2025-03-18T19:31:10.999792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from shapely.geometry import Point\n\ndef df_to_gdf(df, lon_col='Longitude', lat_col='Latitude', crs='EPSG:4326'):\n    \"\"\"\n    Convert a DataFrame to a GeoDataFrame with Point geometry.\n    \"\"\"\n    geometry = [Point(xy) for xy in zip(df[lon_col], df[lat_col])]\n    return gpd.GeoDataFrame(df, geometry=geometry, crs=crs)\n\ntrain_gdf = df_to_gdf(train, lon_col='Longitude', lat_col='Latitude', crs='EPSG:4326')\ntest_gdf = df_to_gdf(test, lon_col='Longitude', lat_col='Latitude', crs='EPSG:4326')\n\ntrain_gdf = train_gdf.to_crs('EPSG:2263')\ntest_gdf  = test_gdf.to_crs('EPSG:2263')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:11.002988Z","iopub.execute_input":"2025-03-18T19:31:11.004363Z","iopub.status.idle":"2025-03-18T19:31:11.164329Z","shell.execute_reply.started":"2025-03-18T19:31:11.004297Z","shell.execute_reply":"2025-03-18T19:31:11.163300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef spatial_join_elevation_chunked_with_rings(df, elev_df, buffer_size=1000, chunk_size=1000):\n    \"\"\"\n    Compute aggregated elevation features for df in chunks to reduce memory usage,\n    including both outer-buffer features and annular ring features.\n    \n    For the outer buffer, we compute:\n      - count, mean, max, min, std, elev_range, and slope_est.\n    For the ring (outer buffer minus inner buffer, with inner = buffer_size/2),\n    we compute:\n      - count, mean, max, std, then ring_elev_range = (max - mean) and slope_est_ring.\n    (We omit the 'min' statistic for the ring features.)\n    \n    Parameters:\n      - df: GeoDataFrame of points in EPSG:2263.\n      - elev_df: GeoDataFrame with elevation data (must contain an 'ELEVATION' column) in EPSG:2263.\n      - buffer_size: Outer buffer radius in meters.\n      - chunk_size: Number of rows to process at a time.\n      \n    Returns:\n      - A DataFrame with aggregated elevation features (both outer and ring) indexed by df's index.\n        Outer columns are named, e.g., \"elev_count_1000m\", \"mean_elev_1000m\", etc.\n        Ring columns are named with a \"_ring\" suffix, e.g., \"elev_count_1000m_ring\", \"mean_elev_1000m_ring\", etc.\n    \"\"\"\n    aggregated_chunks = []\n    total = len(df)\n    \n    # Process df in chunks\n    for start in range(0, total, chunk_size):\n        end = start + chunk_size\n        print(f\"Processing rows {start} to {end} for buffer {buffer_size} m\")\n        chunk = df.iloc[start:end].copy()\n        \n        # --- Outer Buffer Aggregation ---\n        # Create outer buffer geometry\n        outer_buffer = chunk.geometry.buffer(buffer_size)\n        chunk_outer = chunk.copy()\n        chunk_outer[\"geometry_buffer\"] = outer_buffer\n        chunk_outer = chunk_outer.set_geometry(\"geometry_buffer\")\n        \n        # Spatial join for outer buffer\n        joined_outer = gpd.sjoin(chunk_outer, elev_df[['geometry', 'ELEVATION']], how=\"left\", predicate=\"intersects\")\n        joined_outer[\"dummy\"] = 1\n        \n        # Aggregate outer features: count, mean, max, min, std\n        agg_dict_outer = {\"ELEVATION\": ['count', 'mean', 'max', 'min', 'std']}\n        outer_agg = joined_outer.groupby(joined_outer.index).agg(agg_dict_outer)\n        outer_agg.columns = outer_agg.columns.to_flat_index()\n        # Compute additional features for outer buffer\n        outer_agg[\"elev_range\"] = outer_agg[(\"ELEVATION\", \"max\")] - outer_agg[(\"ELEVATION\", \"min\")]\n        outer_agg[\"slope_est\"] = np.degrees(np.arctan(outer_agg[\"elev_range\"] / (2 * buffer_size)))\n        # Rename outer columns\n        rename_map_outer = {\n            (\"ELEVATION\", \"count\"): f\"elev_count_{buffer_size}m\",\n            (\"ELEVATION\", \"mean\"): f\"mean_elev_{buffer_size}m\",\n            (\"ELEVATION\", \"max\"): f\"max_elev_{buffer_size}m\",\n            (\"ELEVATION\", \"min\"): f\"min_elev_{buffer_size}m\",\n            (\"ELEVATION\", \"std\"): f\"std_elev_{buffer_size}m\",\n            \"elev_range\": f\"elev_range_{buffer_size}m\",\n            \"slope_est\": f\"slope_est_{buffer_size}m\"\n        }\n        outer_agg = outer_agg.rename(columns=rename_map_outer)\n        outer_agg = outer_agg.reindex(chunk.index, fill_value=0)\n        \n        # --- Ring (Annulus) Buffer Aggregation ---\n        # Compute inner buffer (half of outer) and ring geometry\n        inner_buffer = chunk.geometry.buffer(buffer_size / 2)\n        ring_geom = outer_buffer.difference(inner_buffer)\n        \n        chunk_ring = chunk.copy()\n        chunk_ring[\"ring_buffer\"] = ring_geom\n        chunk_ring = chunk_ring.set_geometry(\"ring_buffer\")\n        \n        # Spatial join for ring buffer\n        joined_ring = gpd.sjoin(chunk_ring, elev_df[['geometry', 'ELEVATION']], how=\"left\", predicate=\"intersects\")\n        joined_ring[\"dummy\"] = 1\n        \n        # For ring features, we do not compute 'min'. So use count, mean, max, std.\n        agg_dict_ring = {\"ELEVATION\": ['count', 'mean', 'max', 'std']}\n        ring_agg = joined_ring.groupby(joined_ring.index).agg(agg_dict_ring)\n        ring_agg.columns = ring_agg.columns.to_flat_index()\n        # Compute ring elevation range as (max - mean) and slope estimate accordingly\n        ring_agg[\"elev_range\"] = ring_agg[(\"ELEVATION\", \"max\")] - ring_agg[(\"ELEVATION\", \"mean\")]\n        ring_agg[\"slope_est\"] = np.degrees(np.arctan(ring_agg[\"elev_range\"] / (2 * buffer_size)))\n        # Rename ring columns (omit min)\n        rename_map_ring = {\n            (\"ELEVATION\", \"count\"): f\"elev_count_{buffer_size}m_ring\",\n            (\"ELEVATION\", \"mean\"): f\"mean_elev_{buffer_size}m_ring\",\n            (\"ELEVATION\", \"max\"): f\"max_elev_{buffer_size}m_ring\",\n            (\"ELEVATION\", \"std\"): f\"std_elev_{buffer_size}m_ring\",\n            \"elev_range\": f\"elev_range_{buffer_size}m_ring\",\n            \"slope_est\": f\"slope_est_{buffer_size}m_ring\"\n        }\n        ring_agg = ring_agg.rename(columns=rename_map_ring)\n        ring_agg = ring_agg.reindex(chunk.index, fill_value=0)\n        \n        # Combine outer and ring aggregated features\n        agg_chunk = pd.concat([outer_agg, ring_agg], axis=1)\n        \n        # Fill NaN values with 0 for this chunk (already done via reindex but to be safe)\n        agg_chunk = agg_chunk.fillna(0)\n        \n        aggregated_chunks.append(agg_chunk)\n        \n        # Clean up temporary variables\n        del chunk, chunk_outer, chunk_ring, outer_buffer, inner_buffer, ring_geom, joined_outer, joined_ring, outer_agg, ring_agg, agg_chunk\n        gc.collect()\n    \n    # Concatenate results from all chunks and sort by index\n    result = pd.concat(aggregated_chunks)\n    result = result.sort_index()\n    return result\n\n# Example usage:\n# Assume train_gdf and test_gdf are your GeoDataFrames in EPSG:2263,\n# and nyc_elevation is your elevation GeoDataFrame in EPSG:2263 with an 'ELEVATION' column.\nbuffer_list = [500,\n                1000, 1500, 3500, 4000, 4500, 5000, 5500, 8500, 10000, 11000\n              ]\n\ndef process_buffers_chunked_with_rings(df, elev_df, buffer_list, chunk_size=1000):\n    df_final = df.copy()\n    for buffer_size in buffer_list:\n        print(f\"\\n--- Processing buffer: {buffer_size} m ---\")\n        agg_features = spatial_join_elevation_chunked_with_rings(df, elev_df, buffer_size, chunk_size)\n        # Merge aggregated features into the final DataFrame.\n        df_final = df_final.join(agg_features)\n        del agg_features\n        gc.collect()\n    return df_final\n\nprint(\"Processing training elevation features with chunking and rings...\")\ntrain_elev_features = process_buffers_chunked_with_rings(train_gdf, nyc_elevation, buffer_list, chunk_size=1000)\nprint(\"Processing test elevation features with chunking and rings...\")\ntest_elev_features  = process_buffers_chunked_with_rings(test_gdf, nyc_elevation, buffer_list, chunk_size=1000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:11.165456Z","iopub.execute_input":"2025-03-18T19:31:11.166023Z","iopub.status.idle":"2025-03-18T19:31:42.551238Z","shell.execute_reply.started":"2025-03-18T19:31:11.165988Z","shell.execute_reply":"2025-03-18T19:31:42.550073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_elev_features.columns\nelev_cols = [col for col in train_elev_features.columns if col!='Longitude' and  col!='Latitude' and  col!='datetime'\n             and col!='UHI Index' and col!='geometry']\ntrain_elev_features[elev_cols].to_csv('train_elevation_data.csv', index=False)\ntest_elev_features[elev_cols].to_csv('test_elevation_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.552171Z","iopub.execute_input":"2025-03-18T19:31:42.552467Z","iopub.status.idle":"2025-03-18T19:31:42.767979Z","shell.execute_reply.started":"2025-03-18T19:31:42.552440Z","shell.execute_reply":"2025-03-18T19:31:42.766942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pd.set_option('display.max_columns', None)\n# train_elev_features[elev_cols].isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.768992Z","iopub.execute_input":"2025-03-18T19:31:42.769339Z","iopub.status.idle":"2025-03-18T19:31:42.773463Z","shell.execute_reply.started":"2025-03-18T19:31:42.769304Z","shell.execute_reply":"2025-03-18T19:31:42.772399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_elev = gpd.sjoin_nearest(train_gdf, nyc_elevation[['geometry', 'ELEVATION']], how='left', distance_col='dist')\n# test_elev  = gpd.sjoin_nearest(test_gdf,  nyc_elevation[['geometry', 'ELEVATION']], how='left', distance_col='dist')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.774612Z","iopub.execute_input":"2025-03-18T19:31:42.775148Z","iopub.status.idle":"2025-03-18T19:31:42.797309Z","shell.execute_reply.started":"2025-03-18T19:31:42.775099Z","shell.execute_reply":"2025-03-18T19:31:42.796077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_elev = train_elev.loc[~train_elev.index.duplicated(keep='first')]\n# test_elev  = test_elev.loc[~test_elev.index.duplicated(keep='first')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.798533Z","iopub.execute_input":"2025-03-18T19:31:42.798965Z","iopub.status.idle":"2025-03-18T19:31:42.822002Z","shell.execute_reply.started":"2025-03-18T19:31:42.798925Z","shell.execute_reply":"2025-03-18T19:31:42.820663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# from scipy.interpolate import griddata\n\n# points = train_elev[['Longitude', 'Latitude']].values\n# elevations = train_elev['ELEVATION'].values\n\n# min_x, max_x = train_elev['Longitude'].min(), train['Longitude'].max()\n# min_y, max_y = train_elev['Latitude'].min(), train['Latitude'].max()\n\n# grid_x, grid_y = np.mgrid[min_x:max_x:100j, min_y:max_y:100j]\n# grid_elev = griddata(points, elevations, (grid_x, grid_y), method='cubic')\n\n# dz_dx, dz_dy = np.gradient(grid_elev, (max_x-min_x) / 100, (max_y-min_y)/100)\n\n# slope_rad = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))\n# slope_deg = np.degrees(slope_rad)\n\n# plt.figure(figsize = (12, 5))\n# plt.subplot(1,2,1)\n# plt.title(('Interpolated DEM'))\n# plt.imshow(grid_elev.T, extent=(min_x, max_x, min_y, max_y), origin='lower', cmap='terrain')\n# plt.colorbar(label='ELEVATION (m)')\n\n# plt.subplot(1,2,2)\n# plt.title(\"Slope (d)\")\n# plt.imshow(slope_deg.T, extent=(min_x, max_x, min_y, max_y), origin='lower', cmap='viridis')\n# plt.colorbar(label='Slope (d)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.823127Z","iopub.execute_input":"2025-03-18T19:31:42.823444Z","iopub.status.idle":"2025-03-18T19:31:42.848039Z","shell.execute_reply.started":"2025-03-18T19:31:42.823416Z","shell.execute_reply":"2025-03-18T19:31:42.846787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import geopandas as gpd\n# import matplotlib.pyplot as plt\n\n# from shapely.geometry import Point\n# from scipy.interpolate import griddata, RegularGridInterpolator\n\n# def compute_slope_and_plot(\n#     df,\n#     lat_col='Latitude',\n#     lon_col='Longitude',\n#     elev_col='ELEVATION',\n#     resolution=200,\n#     method='linear'\n# ):\n\n#     # -------------------------------------------------------------------------\n#     # 1) Convert df to a GeoDataFrame in EPSG:4326 and reproject to EPSG:2263\n#     # -------------------------------------------------------------------------\n#     gdf_4326 = gpd.GeoDataFrame(\n#         df.copy(),\n#         geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n#         crs=\"EPSG:4326\"\n#     )\n#     # Reproject to EPSG:2263 (NY State Plane, in meters)\n#     gdf_2263 = gdf_4326.to_crs(\"EPSG:2263\")\n\n#     # Extract x, y, and elevation in projected coordinates\n#     x = gdf_2263.geometry.x.values\n#     y = gdf_2263.geometry.y.values\n#     elev = gdf_2263[elev_col].values\n\n#     # -------------------------------------------------------------------------\n#     # 2) Create a bounding box and generate a regular grid\n#     #    with the desired resolution (e.g., 100 => 100x100 grid)\n#     # -------------------------------------------------------------------------\n#     min_x, max_x = x.min(), x.max()\n#     min_y, max_y = y.min(), y.max()\n\n#     # Use np.mgrid to create a 2D grid\n#     grid_x, grid_y = np.mgrid[\n#         min_x:max_x:complex(resolution),\n#         min_y:max_y:complex(resolution)\n#     ]\n\n#     # -------------------------------------------------------------------------\n#     # 3) Interpolate elevation onto the grid\n#     # -------------------------------------------------------------------------\n#     grid_elev = griddata(\n#         points=(x, y),\n#         values=elev,\n#         xi=(grid_x, grid_y),\n#         method=method\n#     )\n\n#     # -------------------------------------------------------------------------\n#     # 4) Compute slope from the interpolated DEM\n#     #    (slope = arctan( sqrt( (dz/dx)^2 + (dz/dy)^2 ) ) )\n#     # -------------------------------------------------------------------------\n#     # Grid spacing in each dimension\n#     dx = (max_x - min_x) / (resolution - 1)\n#     dy = (max_y - min_y) / (resolution - 1)\n\n#     dz_dx, dz_dy = np.gradient(grid_elev, dx, dy)\n#     slope_rad = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))\n#     slope_deg = np.degrees(slope_rad)\n\n#     # -------------------------------------------------------------------------\n#     # 5) Plot the interpolated DEM and slope\n#     # -------------------------------------------------------------------------\n#     plt.figure(figsize=(12, 5))\n\n#     # Interpolated DEM\n#     plt.subplot(1, 2, 1)\n#     plt.title(\"Interpolated DEM\")\n#     plt.imshow(\n#         grid_elev.T,\n#         extent=(min_x, max_x, min_y, max_y),\n#         origin='lower',\n#         cmap='terrain'\n#     )\n#     plt.colorbar(label='Elevation (m)')\n\n#     # Slope\n#     plt.subplot(1, 2, 2)\n#     plt.title(\"Slope (degrees)\")\n#     plt.imshow(\n#         slope_deg.T,\n#         extent=(min_x, max_x, min_y, max_y),\n#         origin='lower',\n#         cmap='viridis'\n#     )\n#     plt.colorbar(label='Slope (Â°)')\n\n#     plt.tight_layout()\n#     plt.show()\n\n#     # -------------------------------------------------------------------------\n#     # 6) Sample slope back to each point using RegularGridInterpolator\n#     # -------------------------------------------------------------------------\n#     # We define 1D arrays for x and y so that the interpolator knows the grid\n#     x_vals = np.linspace(min_x, max_x, resolution)\n#     y_vals = np.linspace(min_y, max_y, resolution)\n\n#     # slope_deg has shape (resolution, resolution), where\n#     # slope_deg[i, j] corresponds to x_vals[i], y_vals[j].\n#     slope_interpolator = RegularGridInterpolator(\n#         (x_vals, y_vals),\n#         slope_deg,\n#         bounds_error=False,\n#         fill_value=np.nan\n#     )\n\n#     # We must pass coordinates as (x, y), so let's build them from gdf_2263\n#     sample_pts = np.column_stack([x, y])  # shape (N,2)\n#     sampled_slope = slope_interpolator(sample_pts)\n\n#     # Add slope back to original df\n#     df_out = df.copy()\n#     df_out['slope_deg'] = sampled_slope\n\n#     return df_out\n\n# train_elev = compute_slope_and_plot(train_elev)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.852074Z","iopub.execute_input":"2025-03-18T19:31:42.852443Z","iopub.status.idle":"2025-03-18T19:31:42.876101Z","shell.execute_reply.started":"2025-03-18T19:31:42.852414Z","shell.execute_reply":"2025-03-18T19:31:42.874931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_elev = compute_slope_and_plot(test_elev)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.878645Z","iopub.execute_input":"2025-03-18T19:31:42.879089Z","iopub.status.idle":"2025-03-18T19:31:42.899133Z","shell.execute_reply.started":"2025-03-18T19:31:42.879041Z","shell.execute_reply":"2025-03-18T19:31:42.898029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import seaborn as sns\n# sns.kdeplot(test_elev['slope_deg'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.900195Z","iopub.execute_input":"2025-03-18T19:31:42.900561Z","iopub.status.idle":"2025-03-18T19:31:42.917663Z","shell.execute_reply.started":"2025-03-18T19:31:42.900525Z","shell.execute_reply":"2025-03-18T19:31:42.916415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import geopandas as gpd\n# import matplotlib.pyplot as plt\n\n# from shapely.geometry import Point\n# from shapely.ops import unary_union\n# from scipy.interpolate import griddata\n\n# # We'll assume you already read your CSVs:\n# #   - train_elev  (with columns ['Longitude','Latitude','ELEVATION',...])\n# #   - test_elev\n# # and you have your NYC elevation data as well.\n\n# # 1) Convert train_elev to a GeoDataFrame in EPSG:4326\n# train_elev_gdf = gpd.GeoDataFrame(\n#     train_elev,\n#     geometry=gpd.points_from_xy(train_elev['Longitude'], train_elev['Latitude']),\n#     crs=\"EPSG:4326\"\n# )\n\n# # 2) Reproject to EPSG:2263 (NY State Plane, units in meters)\n# train_elev_gdf = train_elev_gdf.to_crs(\"EPSG:2263\")\n\n# # Do the same for test_elev if you have it:\n# test_elev_gdf = gpd.GeoDataFrame(\n#     test_elev,\n#     geometry=gpd.points_from_xy(test_elev['Longitude'], test_elev['Latitude']),\n#     crs=\"EPSG:4326\"\n# ).to_crs(\"EPSG:2263\")\n\n# # 3) Convert your NYC elevation points (the big dataset) to GeoDataFrame in EPSG:2263\n# #    (Assuming you already did something like 'nyc_elevation' -> reproject to EPSG:2263)\n# # nyc_elevation = nyc_elevation.to_crs(\"EPSG:2263\")  # if not already done\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.919015Z","iopub.execute_input":"2025-03-18T19:31:42.919460Z","iopub.status.idle":"2025-03-18T19:31:42.939438Z","shell.execute_reply.started":"2025-03-18T19:31:42.919412Z","shell.execute_reply":"2025-03-18T19:31:42.938137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# elev_points = np.array([(geom.x, geom.y) for geom in nyc_elevation.geometry])\n# elev_values = nyc_elevation['ELEVATION'].values\n\n# print(\"Interpolating DEM at 30m resolution...\")\n# grid_elev = griddata(\n#     elev_points,\n#     elev_values,\n#     (grid_x, grid_y),\n#     method='linear'  # or 'cubic'\n# )\n# print(\"DEM interpolation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.940832Z","iopub.execute_input":"2025-03-18T19:31:42.941217Z","iopub.status.idle":"2025-03-18T19:31:42.966014Z","shell.execute_reply.started":"2025-03-18T19:31:42.941180Z","shell.execute_reply":"2025-03-18T19:31:42.964802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grid_elev[grid_elev>0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.966964Z","iopub.execute_input":"2025-03-18T19:31:42.967544Z","iopub.status.idle":"2025-03-18T19:31:42.993790Z","shell.execute_reply.started":"2025-03-18T19:31:42.967509Z","shell.execute_reply":"2025-03-18T19:31:42.992503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dz_dx, dz_dy = np.gradient(grid_elev, res, res)  \n# slope_rad = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))\n# slope_deg = np.degrees(slope_rad)\n# aspect_rad = np.arctan2(-dz_dy, dz_dx)\n# aspect_deg = np.degrees(aspect_rad)\n# # Normalize to [0, 360)\n# aspect_deg = (aspect_deg + 360) % 360\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:42.994955Z","iopub.execute_input":"2025-03-18T19:31:42.995289Z","iopub.status.idle":"2025-03-18T19:31:43.016111Z","shell.execute_reply.started":"2025-03-18T19:31:42.995249Z","shell.execute_reply":"2025-03-18T19:31:43.014936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.neighbors import NearestNeighbors\n\n# n_neighbors = 5\n\n# coords = train_elev[['Longitude', 'Latitude']].values\n\n# nbrs = NearestNeighbors(n_neighbors=n_neighbors+1, algorithm='ball_tree').fit(coords)\n# distances, indices = nbrs.kneighbors(coords)\n\n# local_slopes = []\n# for i in range(len(train_elev)):\n#     neighbor_indices = indices[i][1:]\n#     neighbor_distances = distances[i][1:]\n\n#     elev_diff = np.abs(train_elev.iloc[i]['ELEVATION'] - train_elev.iloc[neighbor_indices['ELEVATION']])\n\n#     slopes = np.arctan(elev_tiff / neighbor_distances)\n#     avg_slope_deg = np.degrees(np.mean(slopes))\n#     local_slopes.append(avg_slope_deg)\n\n# train_elev['local_slope_deg'] = local_slopes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.017233Z","iopub.execute_input":"2025-03-18T19:31:43.017681Z","iopub.status.idle":"2025-03-18T19:31:43.036212Z","shell.execute_reply.started":"2025-03-18T19:31:43.017634Z","shell.execute_reply":"2025-03-18T19:31:43.035092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input_cols = ['ELEVATION', 'dist', 'slope_deg']\n\n# train_elev[input_cols].to_csv('train_elev.csv', index=False)\n# test_elev[input_cols].to_csv('test_elev.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.037367Z","iopub.execute_input":"2025-03-18T19:31:43.037662Z","iopub.status.idle":"2025-03-18T19:31:43.063191Z","shell.execute_reply.started":"2025-03-18T19:31:43.037636Z","shell.execute_reply":"2025-03-18T19:31:43.061835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_elev[input_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.064435Z","iopub.execute_input":"2025-03-18T19:31:43.064798Z","iopub.status.idle":"2025-03-18T19:31:43.084298Z","shell.execute_reply.started":"2025-03-18T19:31:43.064737Z","shell.execute_reply":"2025-03-18T19:31:43.082902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_elev.columns\n# target = train_elev['UHI Index']\n# input_cols = ['ELEVATION', 'dist']\n\n# import xgboost as xgb\n# xgb_reg = xgb.XGBRegressor()\n# xgb_reg.fit(train_elev[input_cols], target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.085526Z","iopub.execute_input":"2025-03-18T19:31:43.085964Z","iopub.status.idle":"2025-03-18T19:31:43.103529Z","shell.execute_reply.started":"2025-03-18T19:31:43.085924Z","shell.execute_reply":"2025-03-18T19:31:43.102366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Elevation bounds:\", nyc_elevation.total_bounds)\n# print(\"Train bounds:\", train_gdf.total_bounds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.105076Z","iopub.execute_input":"2025-03-18T19:31:43.105463Z","iopub.status.idle":"2025-03-18T19:31:43.124074Z","shell.execute_reply.started":"2025-03-18T19:31:43.105424Z","shell.execute_reply":"2025-03-18T19:31:43.122863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Tree, Forestry, Traffic data loading and prep","metadata":{}},{"cell_type":"code","source":"%time\nstreet_tree = pd.read_csv('/kaggle/input/ey-tree-traffic-data-nyc/2015_Street_Tree_Census_-_Tree_Data_20250220.csv')\nauto_traffic = pd.read_csv('/kaggle/input/ey-tree-traffic-data-nyc/Automated_Traffic_Volume_Counts_20250220.csv')\nforestry_tree = pd.read_csv('/kaggle/input/ey-tree-traffic-data-nyc/Forestry_Tree_Points_20250220.csv')\ntraffic = pd.read_csv('/kaggle/input/ey-tree-traffic-data-nyc/Traffic_Volume_Counts_20250220.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:31:43.125120Z","iopub.execute_input":"2025-03-18T19:31:43.125387Z","iopub.status.idle":"2025-03-18T19:32:05.084136Z","shell.execute_reply.started":"2025-03-18T19:31:43.125365Z","shell.execute_reply":"2025-03-18T19:32:05.082710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forestry_tree.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:05.085168Z","iopub.execute_input":"2025-03-18T19:32:05.085445Z","iopub.status.idle":"2025-03-18T19:32:05.673208Z","shell.execute_reply.started":"2025-03-18T19:32:05.085420Z","shell.execute_reply":"2025-03-18T19:32:05.672276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf = df_to_gdf(street_tree, lon_col='longitude', lat_col='latitude', crs='EPSG:4326')\nstree_tree_gdf = stree_tree_gdf.to_crs('EPSG:2263')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:05.674084Z","iopub.execute_input":"2025-03-18T19:32:05.674334Z","iopub.status.idle":"2025-03-18T19:32:11.519865Z","shell.execute_reply.started":"2025-03-18T19:32:05.674312Z","shell.execute_reply":"2025-03-18T19:32:11.518996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf = stree_tree_gdf[stree_tree_gdf['status']=='Alive']\nstree_tree_gdf = stree_tree_gdf[stree_tree_gdf['borough'].isin(['Bronx','Manhattan'])]\nstree_tree_gdf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:11.520859Z","iopub.execute_input":"2025-03-18T19:32:11.521142Z","iopub.status.idle":"2025-03-18T19:32:11.940408Z","shell.execute_reply.started":"2025-03-18T19:32:11.521116Z","shell.execute_reply":"2025-03-18T19:32:11.939266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:11.941300Z","iopub.execute_input":"2025-03-18T19:32:11.941570Z","iopub.status.idle":"2025-03-18T19:32:12.292602Z","shell.execute_reply.started":"2025-03-18T19:32:11.941537Z","shell.execute_reply":"2025-03-18T19:32:12.291637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_gdf.crs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.293623Z","iopub.execute_input":"2025-03-18T19:32:12.293960Z","iopub.status.idle":"2025-03-18T19:32:12.300871Z","shell.execute_reply.started":"2025-03-18T19:32:12.293933Z","shell.execute_reply":"2025-03-18T19:32:12.299801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_gdf = train_gdf.to_crs('EPSG:2263')\n# test_gdf  = test_gdf.to_crs('EPSG:2263')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.302087Z","iopub.execute_input":"2025-03-18T19:32:12.302408Z","iopub.status.idle":"2025-03-18T19:32:12.319914Z","shell.execute_reply.started":"2025-03-18T19:32:12.302381Z","shell.execute_reply":"2025-03-18T19:32:12.318905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.320916Z","iopub.execute_input":"2025-03-18T19:32:12.321281Z","iopub.status.idle":"2025-03-18T19:32:12.359611Z","shell.execute_reply.started":"2025-03-18T19:32:12.321242Z","shell.execute_reply":"2025-03-18T19:32:12.358646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_gdf['geometry_buffer'] = train_gdf.geometry.buffer(5000)\n# test_gdf['geometry_buffer']  = test_gdf.geometry.buffer(5000)\n# stree_tree_gdf['geometry'] = stree_tree_gdf['geometry'].apply(\n#     lambda x: shape(x) if x.geom_type == 'FeatureCollection' else x\n# )\n# train_strees = gpd.sjoin(train_gdf[['geometry']], stree_tree_gdf, how='left', op='within')\n# test_strees  = gpd.sjoin(test_gdf[['geometry']], stree_tree_gdf, how='left', op='within')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.360552Z","iopub.execute_input":"2025-03-18T19:32:12.360844Z","iopub.status.idle":"2025-03-18T19:32:12.382786Z","shell.execute_reply.started":"2025-03-18T19:32:12.360819Z","shell.execute_reply":"2025-03-18T19:32:12.381585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_strees.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.383797Z","iopub.execute_input":"2025-03-18T19:32:12.384163Z","iopub.status.idle":"2025-03-18T19:32:12.405014Z","shell.execute_reply.started":"2025-03-18T19:32:12.384136Z","shell.execute_reply":"2025-03-18T19:32:12.403653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_gdf.crs)        # e.g., EPSG:4326 (WGS84)\nprint(stree_tree_gdf.crs)   # e.g., EPSG:2263 (NAD83 / New York Long Island)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.406486Z","iopub.execute_input":"2025-03-18T19:32:12.406926Z","iopub.status.idle":"2025-03-18T19:32:12.428519Z","shell.execute_reply.started":"2025-03-18T19:32:12.406882Z","shell.execute_reply":"2025-03-18T19:32:12.427363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots()\ntrain_gdf.head(1).buffer(50).plot(ax=ax, color='red', alpha=0.3)  \nstree_tree_gdf.plot(ax=ax, color='green', markersize=1)           \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:12.429709Z","iopub.execute_input":"2025-03-18T19:32:12.430045Z","iopub.status.idle":"2025-03-18T19:32:16.668970Z","shell.execute_reply.started":"2025-03-18T19:32:12.430018Z","shell.execute_reply":"2025-03-18T19:32:16.667811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.670062Z","iopub.execute_input":"2025-03-18T19:32:16.670432Z","iopub.status.idle":"2025-03-18T19:32:16.677345Z","shell.execute_reply.started":"2025-03-18T19:32:16.670398Z","shell.execute_reply":"2025-03-18T19:32:16.675993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stree_tree_gdf['borough'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.678200Z","iopub.execute_input":"2025-03-18T19:32:16.678555Z","iopub.status.idle":"2025-03-18T19:32:16.720846Z","shell.execute_reply.started":"2025-03-18T19:32:16.678523Z","shell.execute_reply":"2025-03-18T19:32:16.719586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_strees = gpd.sjoin_nearest(train_gdf, stree_tree_gdf[['geometry', 'health', 'status', 'tree_dbh', 'curb_loc', 'spc_common']], how='left', distance_col='dist')\n# test_strees  = gpd.sjoin_nearest(test_gdf,  stree_tree_gdf[['geometry', 'health', 'status', 'tree_dbh', 'curb_loc', 'spc_common']], how='left', distance_col='dist')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.722070Z","iopub.execute_input":"2025-03-18T19:32:16.722450Z","iopub.status.idle":"2025-03-18T19:32:16.742107Z","shell.execute_reply.started":"2025-03-18T19:32:16.722411Z","shell.execute_reply":"2025-03-18T19:32:16.740741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_strees[train_strees['dist'] > 200]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.748603Z","iopub.execute_input":"2025-03-18T19:32:16.749029Z","iopub.status.idle":"2025-03-18T19:32:16.762999Z","shell.execute_reply.started":"2025-03-18T19:32:16.748997Z","shell.execute_reply":"2025-03-18T19:32:16.761704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import geopandas as gpd\n# import numpy as np\n\n# # Create buffer and set geometry\n# train_gdf[\"geometry_buffer\"] = train_gdf.geometry.buffer(1000)\n# train_gdf = train_gdf.set_geometry(\"geometry_buffer\")\n\n# # Perform spatial join (intersects)\n# train_strees = gpd.sjoin(\n#     train_gdf,\n#     stree_tree_gdf,\n#     how=\"left\",\n#     predicate=\"intersects\"\n# )\n\n# # Reset geometry to original points\n# train_gdf = train_gdf.set_geometry(\"geometry\")\n\n# # Step 1: Convert health to numerical scores\n# health_mapping = {'Poor': 0, 'Fair': 1, 'Good': 2}\n# train_strees['health_score'] = train_strees['health'].map(health_mapping)\n\n# # Step 2: Ensure tree_dbh is numeric\n# train_strees['tree_dbh'] = train_strees['tree_dbh'].astype(float)\n\n# # Step 3: Update aggregation dictionary to use health_score instead of health\n# agg_dict = {\n#     \"tree_id\": \"count\",          # Number of trees in buffer\n#     \"health_score\": \"mean\",      # Average health score (now numerical)\n#     \"tree_dbh\": \"max\"            # Max tree diameter\n# }\n\n# # Group by original index and aggregate\n# train_agg = train_strees.groupby(train_strees.index).agg(agg_dict)\n\n# # Rename columns for clarity\n# train_agg.columns = [\"tree_count_1000m\", \"avg_tree_health\", \"max_tree_size\"]\n\n# # Merge aggregated data back into original train_gdf\n# train_gdf = train_gdf.join(train_agg)\n\n# # Impute NaN for points with no trees in buffer\n# train_gdf[\"tree_count_1000m\"] = train_gdf[\"tree_count_1000m\"]\n# train_gdf[\"avg_tree_health\"] = train_gdf[\"avg_tree_health\"]\n# train_gdf[\"max_tree_size\"] = train_gdf[\"max_tree_size\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.765318Z","iopub.execute_input":"2025-03-18T19:32:16.765628Z","iopub.status.idle":"2025-03-18T19:32:16.781171Z","shell.execute_reply.started":"2025-03-18T19:32:16.765601Z","shell.execute_reply":"2025-03-18T19:32:16.779907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import geopandas as gpd\n# import numpy as np\n\n# def spatial_join_trees(df, tree_df, buffer_size=1000):\n#     \"\"\"\n#     Perform a spatial join between a dataframe (train/test) and a tree dataset.\n    \n#     Parameters:\n#     - df (GeoDataFrame): The train or test dataframe with geometry.\n#     - tree_df (GeoDataFrame): The tree dataframe with geometry.\n#     - buffer_size (int, optional): The buffer radius for spatial join. Default is 1000 meters.\n    \n#     Returns:\n#     - GeoDataFrame: The input dataframe with aggregated tree data joined.\n#     \"\"\"\n#     df = df.copy()\n    \n#     # Create buffer and set geometry\n#     df[\"geometry_buffer\"] = df.geometry.buffer(buffer_size)\n#     df = df.set_geometry(\"geometry_buffer\")\n    \n#     # Perform spatial join (intersects)\n#     df_trees = gpd.sjoin(df, tree_df, how=\"left\", predicate=\"intersects\")\n    \n#     # Reset geometry to original points\n#     df = df.set_geometry(\"geometry\")\n    \n#     # Convert health to numerical scores\n#     health_mapping = {'Poor': 0, 'Fair': 1, 'Good': 2}\n#     df_trees['health_score'] = df_trees['health'].map(health_mapping)\n    \n#     # Ensure tree_dbh is numeric\n#     df_trees['tree_dbh'] = df_trees['tree_dbh'].astype(float)\n    \n#     # Aggregation dictionary\n#     agg_dict = {\n#         \"tree_id\": \"count\",         # Number of trees in buffer\n#         \"health_score\": \"mean\",     # Average health score (now numerical)\n#         \"tree_dbh\": \"max\"           # Max tree diameter\n#     }\n    \n#     # Group by original index and aggregate\n#     df_agg = df_trees.groupby(df_trees.index).agg(agg_dict)\n    \n#     # Rename columns for clarity\n#     df_agg.columns = [\"tree_count_{}m\".format(buffer_size), \"avg_tree_health\", \"max_tree_size\"]\n    \n#     # Merge aggregated data back into the original df\n#     df = df.join(df_agg)\n    \n#     # Impute NaN for points with no trees in buffer\n#     df[\"tree_count_{}m\".format(buffer_size)] = df[\"tree_count_{}m\".format(buffer_size)].fillna(0)\n#     df[\"avg_tree_health\"] = df[\"avg_tree_health\"].fillna(0)\n#     df[\"max_tree_size\"] = df[\"max_tree_size\"].fillna(0)\n    \n#     return df\n\n# train_stree_1000m = spatial_join_trees(train_gdf, stree_tree_gdf, 1000)\n# test_stree_1000m = spatial_join_trees(test_gdf, stree_tree_gdf, 1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.782343Z","iopub.execute_input":"2025-03-18T19:32:16.782713Z","iopub.status.idle":"2025-03-18T19:32:16.804579Z","shell.execute_reply.started":"2025-03-18T19:32:16.782666Z","shell.execute_reply":"2025-03-18T19:32:16.803236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.1 Street trees cencus 2015","metadata":{}},{"cell_type":"code","source":"import geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef spatial_join_trees_chunked_with_rings(df, tree_df, buffer_size=1000, chunk_size=1000):\n    \"\"\"\n    Compute aggregated tree features for df in chunks (to reduce memory usage),\n    including both outer-buffer features and annular ring features.\n    \n    For the outer buffer, the following are computed:\n      - tree_count: count of tree records (using tree_id)\n      - avg_tree_health: mean of health scores (converted via {'Poor': 0, 'Fair': 1, 'Good': 2})\n      - max_tree_size: maximum tree_dbh\n      \n    For the ring (annulus), we define the inner buffer as half of the buffer_size,\n    then compute the ring geometry as: outer_buffer.difference(inner_buffer).\n    We then compute the same aggregations for the ring, renaming the columns with a _ring suffix.\n    \n    Parameters:\n      - df (GeoDataFrame): Train/test points in EPSG:2263.\n      - tree_df (GeoDataFrame): Tree dataset with columns ['geometry', 'tree_id', 'health', 'tree_dbh'].\n      - buffer_size (int): Buffer radius in meters.\n      - chunk_size (int): Number of rows to process at a time.\n      \n    Returns:\n      - A DataFrame (indexed by df's index) with aggregated tree features for the given buffer.\n        Outer columns are named like \"tree_count_1000m\", \"avg_tree_health_1000m\", etc.\n        Ring columns are named like \"tree_count_1000m_ring\", \"avg_tree_health_1000m_ring\", etc.\n    \"\"\"\n    aggregated_chunks = []\n    total = len(df)\n    \n    # Define health mapping once\n    health_mapping = {'Poor': 0, 'Fair': 1, 'Good': 2}\n    \n    for start in range(0, total, chunk_size):\n        end = start + chunk_size\n        print(f\"Processing rows {start} to {end} for tree buffer {buffer_size} m\")\n        chunk = df.iloc[start:end].copy()\n        \n        # --- Outer Buffer Aggregation ---\n        outer_buffer = chunk.geometry.buffer(buffer_size)\n        chunk_outer = chunk.copy()\n        chunk_outer[\"outer_buffer\"] = outer_buffer\n        chunk_outer = chunk_outer.set_geometry(\"outer_buffer\")\n        \n        joined_outer = gpd.sjoin(chunk_outer, tree_df[['geometry', 'tree_id', 'health', 'tree_dbh']], \n                                 how=\"left\", predicate=\"intersects\")\n        # Reset geometry to original points (not strictly needed for aggregation)\n        chunk_outer = chunk_outer.set_geometry(\"geometry\")\n        \n        # Map health to numeric and ensure tree_dbh is float\n        joined_outer['health_score'] = joined_outer['health'].map(health_mapping)\n        joined_outer['tree_dbh'] = joined_outer['tree_dbh'].astype(float)\n        joined_outer[\"dummy\"] = 1  # for counting\n        \n        agg_dict = {\n            \"tree_id\": \"count\",       # Count trees\n            \"health_score\": \"mean\",   # Average health score\n            \"tree_dbh\": \"max\"         # Maximum tree size (dbh)\n        }\n        outer_agg = joined_outer.groupby(joined_outer.index).agg(agg_dict)\n        # Rename columns for outer features\n        outer_agg.columns = [\n            f\"tree_count_{buffer_size}m\",\n            f\"avg_tree_health_{buffer_size}m\",\n            f\"max_tree_size_{buffer_size}m\"\n        ]\n        outer_agg = outer_agg.reindex(chunk.index, fill_value=0)\n        \n        # --- Ring (Annulus) Aggregation ---\n        # Inner buffer is half of the outer buffer\n        inner_buffer = chunk.geometry.buffer(buffer_size / 2)\n        ring_geom = outer_buffer.difference(inner_buffer)\n        \n        chunk_ring = chunk.copy()\n        chunk_ring[\"ring_buffer\"] = ring_geom\n        chunk_ring = chunk_ring.set_geometry(\"ring_buffer\")\n        \n        joined_ring = gpd.sjoin(chunk_ring, tree_df[['geometry', 'tree_id', 'health', 'tree_dbh']], \n                                how=\"left\", predicate=\"intersects\")\n        joined_ring['health_score'] = joined_ring['health'].map(health_mapping)\n        joined_ring['tree_dbh'] = joined_ring['tree_dbh'].astype(float)\n        joined_ring[\"dummy\"] = 1\n        \n        agg_dict_ring = {\n            \"tree_id\": \"count\",\n            \"health_score\": \"mean\",\n            \"tree_dbh\": \"max\"\n        }\n        ring_agg = joined_ring.groupby(joined_ring.index).agg(agg_dict_ring)\n        ring_agg.columns = [\n            f\"tree_count_{buffer_size}m_ring\",\n            f\"avg_tree_health_{buffer_size}m_ring\",\n            f\"max_tree_size_{buffer_size}m_ring\"\n        ]\n        ring_agg = ring_agg.reindex(chunk.index, fill_value=0)\n        \n        # Combine outer and ring features\n        agg_chunk = pd.concat([outer_agg, ring_agg], axis=1)\n        agg_chunk = agg_chunk.fillna(0)\n        \n        aggregated_chunks.append(agg_chunk)\n        \n        # Cleanup\n        del chunk, chunk_outer, chunk_ring, outer_buffer, inner_buffer, ring_geom, joined_outer, joined_ring, outer_agg, ring_agg, agg_chunk\n        gc.collect()\n    \n    result = pd.concat(aggregated_chunks)\n    result = result.sort_index()\n    return result\n\ndef process_trees_buffers_chunked_with_rings(df, tree_df, buffer_list, chunk_size=1000):\n    \"\"\"\n    For each buffer in buffer_list, compute aggregated tree features (including annular rings)\n    using chunking, then merge them into a single GeoDataFrame.\n    \n    Parameters:\n      - df (GeoDataFrame): Train/test points in EPSG:2263.\n      - tree_df (GeoDataFrame): Tree dataset.\n      - buffer_list (list of int): List of buffer radii in meters.\n      - chunk_size (int): Number of rows to process at a time.\n      \n    Returns:\n      - GeoDataFrame: Original df with additional aggregated tree features for each buffer.\n    \"\"\"\n    df_final = df.copy()\n    for buffer_size in buffer_list:\n        print(f\"\\n--- Processing tree buffer: {buffer_size} m ---\")\n        agg_features = spatial_join_trees_chunked_with_rings(df, tree_df, buffer_size, chunk_size)\n        new_cols = [col for col in agg_features.columns if col.endswith(f\"{buffer_size}m\") or col.endswith(f\"{buffer_size}m_ring\")]\n        df_final = df_final.join(agg_features[new_cols])\n        del agg_features\n        gc.collect()\n    return df_final\n\n# Example usage:\n# Assume train_gdf and test_gdf are your GeoDataFrames (in EPSG:2263),\n# and stree_tree_gdf is your street tree dataset GeoDataFrame.\nbuffer_list = [500,\n                1000, 1500, 3500, 4000, 4500, 5000, 5500, 8500, 10000, 11000\n              ]\n\nprint(\"Processing street tree features for training data...\")\ntrain_stree_features = process_trees_buffers_chunked_with_rings(train_gdf, stree_tree_gdf, buffer_list, chunk_size=1000)\nprint(\"Processing street tree features for test data...\")\ntest_stree_features = process_trees_buffers_chunked_with_rings(test_gdf, stree_tree_gdf, buffer_list, chunk_size=1000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:16.805954Z","iopub.execute_input":"2025-03-18T19:32:16.806380Z","iopub.status.idle":"2025-03-18T19:32:24.412380Z","shell.execute_reply.started":"2025-03-18T19:32:16.806338Z","shell.execute_reply":"2025-03-18T19:32:24.411212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_stree_features.columns\nstree_cols = [col for col in train_stree_features.columns if col!='Longitude' and  col!='Latitude' and  col!='datetime'\n             and col!='UHI Index' and col!='geometry']\ntrain_stree_features[stree_cols].to_csv('train_stree_data.csv', index=False)\ntest_stree_features[stree_cols].to_csv(\"test_stree_data.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:24.413587Z","iopub.execute_input":"2025-03-18T19:32:24.414169Z","iopub.status.idle":"2025-03-18T19:32:24.481904Z","shell.execute_reply.started":"2025-03-18T19:32:24.414112Z","shell.execute_reply":"2025-03-18T19:32:24.480699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Forestry data ","metadata":{}},{"cell_type":"code","source":"import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport shapely.wkt\nimport gc\n\ndef prepare_forestry_data(forestry_df, location_col=\"Location\"):\n    \"\"\"\n    Convert a forestry DataFrame with a WKT location column to a GeoDataFrame in EPSG:2263.\n    \"\"\"\n    forestry_df = forestry_df.copy()\n    forestry_df[\"geometry\"] = forestry_df[location_col].apply(shapely.wkt.loads)\n    forestry_gdf = gpd.GeoDataFrame(forestry_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n    forestry_gdf = forestry_gdf.to_crs(\"EPSG:2263\")\n    return forestry_gdf\n\ndef spatial_join_forestry_chunked_with_rings(df, forestry_gdf, buffer_size=1000, chunk_size=1000):\n    \"\"\"\n    Compute aggregated forestry features for df in chunks, including both outer-buffer and annular ring features.\n    \n    Outer buffer features (for a buffer of radius buffer_size) are:\n      - mean_dbh, max_dbh, mean_risk, max_stump, forestry_count.\n    Annular ring features are computed by subtracting an inner buffer (buffer_size/2) from the outer buffer.\n      - For the ring we compute the same aggregations.\n      \n    Parameters:\n      - df (GeoDataFrame): Points in EPSG:2263.\n      - forestry_gdf (GeoDataFrame): Forestry data in EPSG:2263 (must include 'DBH', 'RiskRating', 'StumpDiameter').\n      - buffer_size (int): Outer buffer radius in meters.\n      - chunk_size (int): Number of rows to process at a time.\n      \n    Returns:\n      - DataFrame: Aggregated forestry features indexed by df's index.\n        Outer columns are named like \"mean_dbh_{buffer_size}m\", etc.\n        Ring columns are named like \"mean_dbh_{buffer_size}m_ring\", etc.\n    \"\"\"\n    aggregated_chunks = []\n    total = len(df)\n    \n    # Define aggregation dictionary for forestry data\n    agg_dict = {\n        \"DBH\": ['mean', 'max'],\n        \"RiskRating\": \"mean\",\n        \"StumpDiameter\": \"max\",\n        \"dummy\": \"sum\"  # for counting\n    }\n    \n    # Process in chunks\n    for start in range(0, total, chunk_size):\n        end = start + chunk_size\n        print(f\"Processing rows {start} to {end} for forestry buffer {buffer_size} m\")\n        chunk = df.iloc[start:end].copy()\n        \n        ### Outer Buffer Aggregation ###\n        outer_buffer = chunk.geometry.buffer(buffer_size)\n        chunk_outer = chunk.copy()\n        chunk_outer[\"geometry_buffer\"] = outer_buffer\n        chunk_outer = chunk_outer.set_geometry(\"geometry_buffer\")\n        \n        # Spatial join with forestry data\n        joined_outer = gpd.sjoin(chunk_outer, forestry_gdf[['geometry', 'DBH', 'RiskRating', 'StumpDiameter']], \n                                 how=\"left\", predicate=\"intersects\")\n        # Reset geometry to original points (for grouping)\n        chunk_outer = chunk_outer.set_geometry(\"geometry\")\n        \n        # Create a dummy column for counts\n        joined_outer[\"dummy\"] = 1\n        \n        # Aggregate outer features\n        outer_agg = joined_outer.groupby(joined_outer.index).agg(agg_dict)\n        outer_agg.columns = outer_agg.columns.to_flat_index()  # flatten MultiIndex\n        \n        # Rename outer columns using your naming convention:\n        rename_map_outer = {\n            (\"DBH\", \"mean\"): f\"mean_dbh_{buffer_size}m\",\n            (\"DBH\", \"max\"): f\"max_dbh_{buffer_size}m\",\n            (\"RiskRating\", \"mean\"): f\"mean_risk_{buffer_size}m\",\n            (\"StumpDiameter\", \"max\"): f\"max_stump_{buffer_size}m\",\n            (\"dummy\", \"sum\"): f\"forestry_count_{buffer_size}m\"\n        }\n        outer_agg = outer_agg.rename(columns=rename_map_outer)\n        outer_agg = outer_agg.reindex(chunk.index, fill_value=0)\n        \n        ### Ring (Annulus) Aggregation ###\n        inner_buffer = chunk.geometry.buffer(buffer_size / 2)\n        ring_geom = outer_buffer.difference(inner_buffer)\n        \n        chunk_ring = chunk.copy()\n        chunk_ring[\"ring_buffer\"] = ring_geom\n        chunk_ring = chunk_ring.set_geometry(\"ring_buffer\")\n        \n        joined_ring = gpd.sjoin(chunk_ring, forestry_gdf[['geometry', 'DBH', 'RiskRating', 'StumpDiameter']], \n                                how=\"left\", predicate=\"intersects\")\n        joined_ring[\"dummy\"] = 1\n        ring_agg = joined_ring.groupby(joined_ring.index).agg(agg_dict)\n        ring_agg.columns = ring_agg.columns.to_flat_index()\n        \n        rename_map_ring = {\n            (\"DBH\", \"mean\"): f\"mean_dbh_{buffer_size}m_ring\",\n            (\"DBH\", \"max\"): f\"max_dbh_{buffer_size}m_ring\",\n            (\"RiskRating\", \"mean\"): f\"mean_risk_{buffer_size}m_ring\",\n            (\"StumpDiameter\", \"max\"): f\"max_stump_{buffer_size}m_ring\",\n            (\"dummy\", \"sum\"): f\"forestry_count_{buffer_size}m_ring\"\n        }\n        ring_agg = ring_agg.rename(columns=rename_map_ring)\n        ring_agg = ring_agg.reindex(chunk.index, fill_value=0)\n        \n        # Combine outer and ring features for this chunk\n        agg_chunk = pd.concat([outer_agg, ring_agg], axis=1)\n        agg_chunk = agg_chunk.fillna(0)\n        aggregated_chunks.append(agg_chunk)\n        \n        # Cleanup\n        del chunk, chunk_outer, chunk_ring, outer_buffer, inner_buffer, ring_geom, joined_outer, joined_ring, outer_agg, ring_agg, agg_chunk\n        gc.collect()\n    \n    result = pd.concat(aggregated_chunks)\n    result = result.sort_index()\n    return result\n\ndef process_forestry_buffers_chunked_with_rings(df, forestry_gdf, buffer_list, chunk_size=1000):\n    \"\"\"\n    For each buffer in buffer_list, compute aggregated forestry features (outer and ring)\n    using chunking, then merge them into a single GeoDataFrame.\n    \n    Parameters:\n      - df (GeoDataFrame): Points in EPSG:2263.\n      - forestry_gdf (GeoDataFrame): Forestry data in EPSG:2263.\n      - buffer_list (list of int): List of buffer radii in meters.\n      - chunk_size (int): Number of rows to process per chunk.\n      \n    Returns:\n      - GeoDataFrame: The original df with additional aggregated forestry features for each buffer.\n    \"\"\"\n    df_final = df.copy()\n    for buffer_size in buffer_list:\n        print(f\"\\n--- Processing forestry buffer: {buffer_size} m ---\")\n        agg_features = spatial_join_forestry_chunked_with_rings(df, forestry_gdf, buffer_size, chunk_size)\n        # Select columns that end with either '{buffer_size}m' or '{buffer_size}m_ring'\n        new_cols = [col for col in agg_features.columns if col.endswith(f\"{buffer_size}m\") or col.endswith(f\"{buffer_size}m_ring\")]\n        df_final = df_final.join(agg_features[new_cols])\n        del agg_features\n        gc.collect()\n    return df_final\n\n# Example usage:\n# Assume train_gdf and test_gdf are your GeoDataFrames in EPSG:2263,\n# and forestry_df is your forestry DataFrame with a WKT location column.\n# First, prepare forestry GeoDataFrame.\nforestry_gdf = prepare_forestry_data(forestry_tree, location_col=\"Location\")\n\n# Specify the list of buffer sizes (in meters)\nbuffer_list = [500,\n                1000, 1500, 3500, 4000, 4500, 5000, 5500, 8500, 10000, 11000\n              ]\n\nprint(\"Processing forestry features for training data...\")\ntrain_forestry_features = process_forestry_buffers_chunked_with_rings(train_gdf, forestry_gdf, buffer_list, chunk_size=500)\nprint(\"Processing forestry features for test data...\")\ntest_forestry_features = process_forestry_buffers_chunked_with_rings(test_gdf, forestry_gdf, buffer_list, chunk_size=500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:32:24.483041Z","iopub.execute_input":"2025-03-18T19:32:24.483441Z","iopub.status.idle":"2025-03-18T19:33:23.738012Z","shell.execute_reply.started":"2025-03-18T19:32:24.483406Z","shell.execute_reply":"2025-03-18T19:33:23.737007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_forestry_features.columns\nforestry_cols = [col for col in train_forestry_features.columns if col!='Longitude' and  col!='Latitude' and  col!='datetime'\n             and col!='UHI Index' and col!='geometry']\ntrain_forestry_features[forestry_cols].to_csv('train_forestry_data.csv', index=False)\ntest_forestry_features[forestry_cols].to_csv('test_forestry_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:23.738850Z","iopub.execute_input":"2025-03-18T19:33:23.739101Z","iopub.status.idle":"2025-03-18T19:33:23.859418Z","shell.execute_reply.started":"2025-03-18T19:33:23.739079Z","shell.execute_reply":"2025-03-18T19:33:23.858301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_forestry_features[forestry_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:23.860483Z","iopub.execute_input":"2025-03-18T19:33:23.860806Z","iopub.status.idle":"2025-03-18T19:33:23.864960Z","shell.execute_reply.started":"2025-03-18T19:33:23.860745Z","shell.execute_reply":"2025-03-18T19:33:23.863840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auto_traffic = auto_traffic[auto_traffic['Yr']==2021]\n# auto_traffic = auto_traffic[auto_traffic['Boro'].isin(['Bronx', 'Manhattan'])]\n# auto_traffic.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:23.865837Z","iopub.execute_input":"2025-03-18T19:33:23.866190Z","iopub.status.idle":"2025-03-18T19:33:23.889342Z","shell.execute_reply.started":"2025-03-18T19:33:23.866163Z","shell.execute_reply":"2025-03-18T19:33:23.888347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_gdf.crs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:23.890704Z","iopub.execute_input":"2025-03-18T19:33:23.891142Z","iopub.status.idle":"2025-03-18T19:33:23.911062Z","shell.execute_reply.started":"2025-03-18T19:33:23.891102Z","shell.execute_reply":"2025-03-18T19:33:23.909727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 Traffic data ","metadata":{}},{"cell_type":"code","source":"new_traffic = auto_traffic[auto_traffic['Boro'].isin(['Bronx', 'Manhattan'])]\nnew_traffic_2016_2022 = new_traffic[new_traffic['Yr'].isin([2016, 2017, 2018, 2019, 2020, 2021, 2022])]\nnew_traffic_2018_2022 = new_traffic[new_traffic['Yr'].isin([2018, 2019, 2020, 2021, 2022])]\nnew_traffic_2019_2022 = new_traffic[new_traffic['Yr'].isin([2019, 2020, 2021, 2022])]\nnew_traffic_2021 = new_traffic[new_traffic['Yr'].isin([2021])]\nnew_traffic_2020_2022 = new_traffic[new_traffic['Yr'].isin([2020, 2021, 2022])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:23.912283Z","iopub.execute_input":"2025-03-18T19:33:23.912663Z","iopub.status.idle":"2025-03-18T19:33:24.066289Z","shell.execute_reply.started":"2025-03-18T19:33:23.912622Z","shell.execute_reply":"2025-03-18T19:33:24.065208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport geopandas as gpd\nfrom shapely import wkt\nimport numpy as np\n\ndef prepare_traffic_data(traffic_csv, wkt_col=\"WktGeom\"):\n    \"\"\"\n    Reads the traffic CSV and converts it to a GeoDataFrame with geometry\n    derived from the WktGeom column. Assumes the input data is already in EPSG:2263.\n    \"\"\"\n    df = traffic_csv.copy()\n    df[\"geometry\"] = df[wkt_col].apply(wkt.loads)\n    gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:2263\")\n    return gdf\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef spatial_join_traffic_chunked_with_rings(df, traffic_gdf, buffer_size=1000, chunk_size=1000):\n    \"\"\"\n    Compute aggregated traffic features for df in chunks, including both outer-buffer\n    and annular ring (outer buffer minus inner buffer) features.\n    \n    Outer features:\n      - traffic_vol: Sum of traffic volumes.\n      - traffic_count: Count of traffic records.\n    \n    Ring features (using an inner buffer of half the radius):\n      - traffic_vol_ring: Sum of traffic volumes within the annulus.\n      - traffic_count_ring: Count of traffic records in the annulus.\n    \n    Parameters:\n      - df: GeoDataFrame of study points (EPSG:2263).\n      - traffic_gdf: Traffic GeoDataFrame (EPSG:2263) with a 'Vol' column.\n      - buffer_size: Buffer radius in meters.\n      - chunk_size: Number of rows to process at a time.\n      \n    Returns:\n      - DataFrame indexed by df's index containing aggregated features:\n          Outer: traffic_vol_{buffer_size}m, traffic_count_{buffer_size}m\n          Ring: traffic_vol_{buffer_size}m_ring, traffic_count_{buffer_size}m_ring\n    \"\"\"\n    aggregated_chunks = []\n    total = len(df)\n    \n    for start in range(0, total, chunk_size):\n        end = start + chunk_size\n        print(f\"Processing rows {start} to {end} for traffic buffer {buffer_size} m\")\n        chunk = df.iloc[start:end].copy()\n        \n        # ---- Outer Buffer Aggregation ----\n        outer_buffer = chunk.geometry.buffer(buffer_size)\n        chunk_outer = chunk.copy()\n        chunk_outer[\"geometry_buffer\"] = outer_buffer\n        chunk_outer = chunk_outer.set_geometry(\"geometry_buffer\")\n        \n        joined_outer = gpd.sjoin(chunk_outer, traffic_gdf[['geometry', 'Vol']], \n                                 how=\"left\", predicate=\"intersects\")\n        # Reset geometry back to original points\n        chunk = chunk.set_geometry(\"geometry\")\n        \n        # Dummy column for counting\n        joined_outer[\"dummy\"] = 1\n        \n        # Aggregate outer features\n        agg_dict = {\"Vol\": \"sum\", \"dummy\": \"sum\"}\n        outer_agg = joined_outer.groupby(joined_outer.index).agg(agg_dict)\n        outer_agg = outer_agg.rename(columns={\n            \"Vol\": f\"traffic_vol_{buffer_size}m\",\n            \"dummy\": f\"traffic_count_{buffer_size}m\"\n        })\n        outer_agg = outer_agg.reindex(chunk.index, fill_value=0)\n        \n        # ---- Ring (Annulus) Aggregation ----\n        # Compute inner buffer (half of buffer_size) and then ring geometry\n        inner_buffer = chunk.geometry.buffer(buffer_size / 2)\n        ring_geom = outer_buffer.difference(inner_buffer)\n        \n        chunk_ring = chunk.copy()\n        chunk_ring[\"ring_buffer\"] = ring_geom\n        chunk_ring = chunk_ring.set_geometry(\"ring_buffer\")\n        \n        joined_ring = gpd.sjoin(chunk_ring, traffic_gdf[['geometry', 'Vol']], \n                                how=\"left\", predicate=\"intersects\")\n        joined_ring[\"dummy\"] = 1\n        \n        ring_agg = joined_ring.groupby(joined_ring.index).agg(agg_dict)\n        ring_agg = ring_agg.rename(columns={\n            \"Vol\": f\"traffic_vol_{buffer_size}m_ring\",\n            \"dummy\": f\"traffic_count_{buffer_size}m_ring\"\n        })\n        ring_agg = ring_agg.reindex(chunk.index, fill_value=0)\n        \n        # Combine outer and ring features\n        agg_chunk = pd.concat([outer_agg, ring_agg], axis=1)\n        agg_chunk = agg_chunk.fillna(0)\n        \n        aggregated_chunks.append(agg_chunk)\n        \n        # Cleanup temporary variables\n        del chunk, chunk_outer, chunk_ring, outer_buffer, inner_buffer, ring_geom, joined_outer, joined_ring, outer_agg, ring_agg, agg_chunk\n        gc.collect()\n    \n    result = pd.concat(aggregated_chunks)\n    result = result.sort_index()\n    return result\n\ndef process_traffic_buffers_chunked_with_rings(df, traffic_gdf, buffer_list, chunk_size=1000):\n    \"\"\"\n    For each buffer in buffer_list, compute aggregated traffic features (outer and ring)\n    using chunking, then merge the results into a single GeoDataFrame.\n    \n    Parameters:\n      - df: GeoDataFrame of study points (EPSG:2263).\n      - traffic_gdf: Traffic GeoDataFrame (EPSG:2263) with a 'Vol' column.\n      - buffer_list: List of buffer radii in meters.\n      - chunk_size: Number of rows to process per chunk.\n      \n    Returns:\n      - GeoDataFrame: The original df with additional aggregated traffic features for each buffer.\n    \"\"\"\n    df_final = df.copy()\n    for buffer_size in buffer_list:\n        print(f\"\\nProcessing traffic buffer: {buffer_size} m\")\n        agg_features = spatial_join_traffic_chunked_with_rings(df, traffic_gdf, buffer_size, chunk_size)\n        # Select columns ending with either '{buffer_size}m' or '{buffer_size}m_ring'\n        new_cols = [col for col in agg_features.columns if col.endswith(f\"{buffer_size}m\") or col.endswith(f\"{buffer_size}m_ring\")]\n        df_final = df_final.join(agg_features[new_cols])\n        del agg_features\n        gc.collect()\n    return df_final\n\n# Example usage:\n# Assume train_gdf and test_gdf are your GeoDataFrames (in EPSG:2263) for training and testing,\n# and new_traffic_2016_2022 is your traffic DataFrame.\ntraffic_gdf = prepare_traffic_data(new_traffic_2019_2022, wkt_col=\"WktGeom\")\nbuffer_list = [500,\n                # 1000, 1500, 3500, 4000, 4500, 5000, 5500, 8500, 10000, 11000\n              ]\n\nprint(\"Processing traffic features for training data...\")\ntrain_traffic_features = process_traffic_buffers_chunked_with_rings(train_gdf, traffic_gdf, buffer_list, chunk_size=1000)\nprint(\"Processing traffic features for test data...\")\ntest_traffic_features = process_traffic_buffers_chunked_with_rings(test_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:24.067328Z","iopub.execute_input":"2025-03-18T19:33:24.067628Z","iopub.status.idle":"2025-03-18T19:33:32.042029Z","shell.execute_reply.started":"2025-03-18T19:33:24.067600Z","shell.execute_reply":"2025-03-18T19:33:32.040916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"traffic_cols = [col for col in train_traffic_features.columns if col!='Latitude' and col!='Longitude' and col!='datetime' and col!='UHI Index' and col!='geometry']\ntrain_traffic_features[traffic_cols].to_csv('train_traffic_2019_2022.csv', index=False)\ntest_traffic_features[traffic_cols].to_csv('test_traffic_2019_2022.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.043142Z","iopub.execute_input":"2025-03-18T19:33:32.043436Z","iopub.status.idle":"2025-03-18T19:33:32.078659Z","shell.execute_reply.started":"2025-03-18T19:33:32.043411Z","shell.execute_reply":"2025-03-18T19:33:32.077667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traffic_gdf = prepare_traffic_data(new_traffic_2018_2022, wkt_col=\"WktGeom\")\n# buffer_list = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,\n#                5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 10000,\n#                 11000, 11500, 12000, 12500, 13000, 13500, 14000]\n\n# print(\"Processing traffic features for training data...\")\n# train_traffic_features = process_traffic_buffers_chunked(train_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n# print(\"Processing traffic features for test data...\")\n# test_traffic_features = process_traffic_buffers_chunked(test_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n\n# traffic_cols = [col for col in train_traffic_features.columns if col!='Latitude' and col!='Longitude' and col!='datetime' and col!='UHI Index' and col!='geometry']\n# train_traffic_features[traffic_cols].to_csv('train_traffic_2018_2022.csv', index=False)\n# test_traffic_features[traffic_cols].to_csv('test_traffic_2018_2022.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.079597Z","iopub.execute_input":"2025-03-18T19:33:32.079918Z","iopub.status.idle":"2025-03-18T19:33:32.084464Z","shell.execute_reply.started":"2025-03-18T19:33:32.079892Z","shell.execute_reply":"2025-03-18T19:33:32.083074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traffic_gdf = prepare_traffic_data(new_traffic_2019_2022, wkt_col=\"WktGeom\")\n# buffer_list = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,\n#                5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 10000,\n#                 11000, 11500, 12000, 12500, 13000, 13500, 14000]\n\n# print(\"Processing traffic features for training data...\")\n# train_traffic_features = process_traffic_buffers_chunked(train_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n# print(\"Processing traffic features for test data...\")\n# test_traffic_features = process_traffic_buffers_chunked(test_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n\n# traffic_cols = [col for col in train_traffic_features.columns if col!='Latitude' and col!='Longitude' and col!='datetime' and col!='UHI Index' and col!='geometry']\n# train_traffic_features[traffic_cols].to_csv('train_traffic_2019_2022.csv', index=False)\n# test_traffic_features[traffic_cols].to_csv('test_traffic_2019_2022.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.085620Z","iopub.execute_input":"2025-03-18T19:33:32.085954Z","iopub.status.idle":"2025-03-18T19:33:32.107334Z","shell.execute_reply.started":"2025-03-18T19:33:32.085927Z","shell.execute_reply":"2025-03-18T19:33:32.106225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traffic_gdf = prepare_traffic_data(new_traffic_2021, wkt_col=\"WktGeom\")\n# buffer_list = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,\n#                5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 10000,\n#                 11000, 11500, 12000, 12500, 13000, 13500, 14000]\n\n# print(\"Processing traffic features for training data...\")\n# train_traffic_features = process_traffic_buffers_chunked(train_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n# print(\"Processing traffic features for test data...\")\n# test_traffic_features = process_traffic_buffers_chunked(test_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n\n# traffic_cols = [col for col in train_traffic_features.columns if col!='Latitude' and col!='Longitude' and col!='datetime' and col!='UHI Index' and col!='geometry']\n# train_traffic_features[traffic_cols].to_csv('train_traffic_2021.csv', index=False)\n# test_traffic_features[traffic_cols].to_csv('test_traffic_2021.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.108479Z","iopub.execute_input":"2025-03-18T19:33:32.108882Z","iopub.status.idle":"2025-03-18T19:33:32.128149Z","shell.execute_reply.started":"2025-03-18T19:33:32.108842Z","shell.execute_reply":"2025-03-18T19:33:32.126824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traffic_gdf = prepare_traffic_data(new_traffic_2020_2022, wkt_col=\"WktGeom\")\n# buffer_list = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000,\n#                5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 10000,\n#                 11000, 11500, 12000, 12500, 13000, 13500, 14000]\n\n# print(\"Processing traffic features for training data...\")\n# train_traffic_features = process_traffic_buffers_chunked(train_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n# print(\"Processing traffic features for test data...\")\n# test_traffic_features = process_traffic_buffers_chunked(test_gdf, traffic_gdf, buffer_list, chunk_size=1000)\n\n# traffic_cols = [col for col in train_traffic_features.columns if col!='Latitude' and col!='Longitude' and col!='datetime' and col!='UHI Index' and col!='geometry']\n# train_traffic_features[traffic_cols].to_csv('train_traffic_2020_2022.csv', index=False)\n# test_traffic_features[traffic_cols].to_csv('test_traffic_2020_2022.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.129359Z","iopub.execute_input":"2025-03-18T19:33:32.129747Z","iopub.status.idle":"2025-03-18T19:33:32.149023Z","shell.execute_reply.started":"2025-03-18T19:33:32.129714Z","shell.execute_reply":"2025-03-18T19:33:32.147690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/working/train_traffic_2021.csv')\n# df.head(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.150223Z","iopub.execute_input":"2025-03-18T19:33:32.150596Z","iopub.status.idle":"2025-03-18T19:33:32.167525Z","shell.execute_reply.started":"2025-03-18T19:33:32.150566Z","shell.execute_reply":"2025-03-18T19:33:32.166464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.168781Z","iopub.execute_input":"2025-03-18T19:33:32.169124Z","iopub.status.idle":"2025-03-18T19:33:32.186231Z","shell.execute_reply.started":"2025-03-18T19:33:32.169094Z","shell.execute_reply":"2025-03-18T19:33:32.185131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# fig, ax = plt.subplots(figsize=(10, 10))\n# train_gdf.plot(ax=ax, color='blue', markersize=5, label='Study Points')\n# traffic_gdf.plot(ax=ax, color='red', markersize=5, label='Traffic Points')\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.187247Z","iopub.execute_input":"2025-03-18T19:33:32.187545Z","iopub.status.idle":"2025-03-18T19:33:32.210184Z","shell.execute_reply.started":"2025-03-18T19:33:32.187518Z","shell.execute_reply":"2025-03-18T19:33:32.209088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_traffic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.211279Z","iopub.execute_input":"2025-03-18T19:33:32.211752Z","iopub.status.idle":"2025-03-18T19:33:32.229879Z","shell.execute_reply.started":"2025-03-18T19:33:32.211688Z","shell.execute_reply":"2025-03-18T19:33:32.228565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_traffic['WktGeom'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.231110Z","iopub.execute_input":"2025-03-18T19:33:32.231426Z","iopub.status.idle":"2025-03-18T19:33:32.246782Z","shell.execute_reply.started":"2025-03-18T19:33:32.231399Z","shell.execute_reply":"2025-03-18T19:33:32.245673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auto_traffic.crs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.247829Z","iopub.execute_input":"2025-03-18T19:33:32.248187Z","iopub.status.idle":"2025-03-18T19:33:32.267500Z","shell.execute_reply.started":"2025-03-18T19:33:32.248151Z","shell.execute_reply":"2025-03-18T19:33:32.266196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# new_traffic['WktGeom'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.268797Z","iopub.execute_input":"2025-03-18T19:33:32.269210Z","iopub.status.idle":"2025-03-18T19:33:32.287994Z","shell.execute_reply.started":"2025-03-18T19:33:32.269180Z","shell.execute_reply":"2025-03-18T19:33:32.286890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_gdf.crs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.289082Z","iopub.execute_input":"2025-03-18T19:33:32.289447Z","iopub.status.idle":"2025-03-18T19:33:32.307612Z","shell.execute_reply.started":"2025-03-18T19:33:32.289409Z","shell.execute_reply":"2025-03-18T19:33:32.306324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traffic_gdf.crs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.308683Z","iopub.execute_input":"2025-03-18T19:33:32.309105Z","iopub.status.idle":"2025-03-18T19:33:32.327102Z","shell.execute_reply.started":"2025-03-18T19:33:32.309062Z","shell.execute_reply":"2025-03-18T19:33:32.326012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auto_traffic['WktGeom'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.328336Z","iopub.execute_input":"2025-03-18T19:33:32.328714Z","iopub.status.idle":"2025-03-18T19:33:32.344135Z","shell.execute_reply.started":"2025-03-18T19:33:32.328678Z","shell.execute_reply":"2025-03-18T19:33:32.343052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# base = train_gdf.plot(color='blue', markersize=5, label=\"Study Points\")\n# train_gdf.buffer(50000).plot(ax=base, facecolor='none', edgecolor='blue', label=\"500m Buffer\")\n# traffic_gdf.plot(ax=base, color='green', markersize=3, label=\"Traffic Points\")\n# plt.legend()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.345272Z","iopub.execute_input":"2025-03-18T19:33:32.345641Z","iopub.status.idle":"2025-03-18T19:33:32.364876Z","shell.execute_reply.started":"2025-03-18T19:33:32.345597Z","shell.execute_reply":"2025-03-18T19:33:32.363594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Study points extent:\", train_gdf.total_bounds)\n# print(\"Traffic data extent:\", traffic_gdf.total_bounds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.366021Z","iopub.execute_input":"2025-03-18T19:33:32.366313Z","iopub.status.idle":"2025-03-18T19:33:32.383396Z","shell.execute_reply.started":"2025-03-18T19:33:32.366286Z","shell.execute_reply":"2025-03-18T19:33:32.382318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check if trees exist within the buffer for the first point\n# sample_buffer = train_gdf.iloc[0].geometry.buffer(10000)  # 10km buffer\n# trees_in_buffer = stree_tree_gdf[stree_tree_gdf.intersects(sample_buffer)]\n# print(f\"Trees in sample buffer: {len(trees_in_buffer)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.384313Z","iopub.execute_input":"2025-03-18T19:33:32.384585Z","iopub.status.idle":"2025-03-18T19:33:32.403262Z","shell.execute_reply.started":"2025-03-18T19:33:32.384561Z","shell.execute_reply":"2025-03-18T19:33:32.402190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_strees.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.404127Z","iopub.execute_input":"2025-03-18T19:33:32.404447Z","iopub.status.idle":"2025-03-18T19:33:32.419899Z","shell.execute_reply.started":"2025-03-18T19:33:32.404417Z","shell.execute_reply":"2025-03-18T19:33:32.418911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# stree_tree_gdf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.420946Z","iopub.execute_input":"2025-03-18T19:33:32.421293Z","iopub.status.idle":"2025-03-18T19:33:32.436038Z","shell.execute_reply.started":"2025-03-18T19:33:32.421264Z","shell.execute_reply":"2025-03-18T19:33:32.434917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# forestry_tree.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.437291Z","iopub.execute_input":"2025-03-18T19:33:32.437651Z","iopub.status.idle":"2025-03-18T19:33:32.452615Z","shell.execute_reply.started":"2025-03-18T19:33:32.437609Z","shell.execute_reply":"2025-03-18T19:33:32.451673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# auto_traffic.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T19:33:32.453665Z","iopub.execute_input":"2025-03-18T19:33:32.454062Z","iopub.status.idle":"2025-03-18T19:33:32.471713Z","shell.execute_reply.started":"2025-03-18T19:33:32.454032Z","shell.execute_reply":"2025-03-18T19:33:32.470656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}