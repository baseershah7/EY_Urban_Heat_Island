{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10580423,"sourceType":"datasetVersion","datasetId":6547867},{"sourceId":10580457,"sourceType":"datasetVersion","datasetId":6547892}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":367.589794,"end_time":"2025-02-06T12:46:49.914948","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-06T12:40:42.325154","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-27T17:56:00.516398Z","iopub.execute_input":"2025-02-27T17:56:00.516740Z","iopub.status.idle":"2025-02-27T17:56:00.934580Z","shell.execute_reply.started":"2025-02-27T17:56:00.516715Z","shell.execute_reply":"2025-02-27T17:56:00.933641Z"},"papermill":{"duration":1.010841,"end_time":"2025-02-06T12:40:46.230501","exception":false,"start_time":"2025-02-06T12:40:45.219660","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install osmnx","metadata":{"execution":{"iopub.status.busy":"2025-02-27T17:56:00.935875Z","iopub.execute_input":"2025-02-27T17:56:00.936280Z","iopub.status.idle":"2025-02-27T17:56:08.026401Z","shell.execute_reply.started":"2025-02-27T17:56:00.936257Z","shell.execute_reply":"2025-02-27T17:56:08.025207Z"},"papermill":{"duration":6.697411,"end_time":"2025-02-06T12:40:52.933893","exception":false,"start_time":"2025-02-06T12:40:46.236482","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/uhi-index-data/Training_data_uhi_index_UHI2025-v2.csv')\ntest_df = pd.read_csv('/kaggle/input/test-df-ey-open-science/Submission_template_UHI2025-v2.csv')","metadata":{"execution":{"iopub.status.busy":"2025-02-27T17:56:08.029066Z","iopub.execute_input":"2025-02-27T17:56:08.029407Z","iopub.status.idle":"2025-02-27T17:56:08.088668Z","shell.execute_reply.started":"2025-02-27T17:56:08.029377Z","shell.execute_reply":"2025-02-27T17:56:08.087334Z"},"papermill":{"duration":0.072798,"end_time":"2025-02-06T12:40:53.017727","exception":false,"start_time":"2025-02-06T12:40:52.944929","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import osmnx as ox\nfrom shapely import geometry as geom\nimport networkx as nx\n# Download water bodies for NYC (polygons)\ndef get_osmnx_features(aoi_polygon):\n    \"\"\"Fetch comprehensive urban features from OSMnx limited to the given polygon.\"\"\"\n    print(\"Fetching comprehensive OSMnx features for AOI...\")\n    tags = {\n        'building': True,\n        'highway': True,\n        'natural': 'water',\n        'leisure': 'park',\n    }\n    gdf = ox.features_from_polygon(aoi_polygon, tags=tags)\n    print(\"Fetched OSMnx features for AOI.\")\n    return gdf\nwater = ox.features_from_place(\"New York City, USA\", tags={\"natural\": \"water\"})\n\n# Download parks (replace with NYC OpenData link for higher accuracy)\nparks = ox.features_from_place(\"New York City, USA\", tags={\"leisure\": \"park\"})\nnyc_bbox = (-74.01, 40.75, -73.86, 40.88)  # in EPSG:4326\nnyc_poly = geom.box(*nyc_bbox)\nG = ox.graph_from_polygon(nyc_poly, network_type='drive')\nroads = ox.graph_to_gdfs(G, nodes=False, edges=True).to_crs(\"EPSG:2263\")\nnodes = ox.graph_to_gdfs(G, nodes=True, edges=False).to_crs(\"EPSG:2263\")\ncloseness_dict = nx.closeness_centrality(G)\nnodes[\"closeness\"] = nodes.index.map(closeness_dict)\n# nyc_features = get_osmnx_features(nyc_poly).to_crs(\"EPSG:2263\")\n# buildings = nyc_features[nyc_features[\"building\"].notna()].copy()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T17:56:08.090349Z","iopub.execute_input":"2025-02-27T17:56:08.090728Z","iopub.status.idle":"2025-02-27T17:59:11.114050Z","shell.execute_reply.started":"2025-02-27T17:56:08.090685Z","shell.execute_reply":"2025-02-27T17:59:11.112890Z"},"papermill":{"duration":26.188216,"end_time":"2025-02-06T12:41:19.213136","exception":false,"start_time":"2025-02-06T12:40:53.024920","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from shapely.geometry import Point\nimport geopandas as gpd\n\n# Convert train/test coordinates to GeoDataFrame\ntrain_geometry = [Point(lon, lat) for lon, lat in zip(train_df[\"Longitude\"], train_df[\"Latitude\"])]\ntest_geometry = [Point(lon, lat) for lon, lat in zip(test_df[\"Longitude\"], test_df[\"Latitude\"])]\ntrain_gdf = gpd.GeoDataFrame(train_df, geometry=train_geometry, crs=\"EPSG:4326\")\ntrain_gdf = train_gdf.to_crs('EPSG:2263')\ntest_gdf = gpd.GeoDataFrame(test_df, geometry=test_geometry, crs=\"EPSG:4326\")\ntest_gdf = test_gdf.to_crs('EPSG:2263')\n# # Compute distance to nearest water body (meters)\n# train_gdf[\"distance_to_water\"] = train_gdf.geometry.apply(\n#     lambda x: water.geometry.distance(x).min()\n# )\n# test_gdf[\"distance_to_water\"] = test_gdf.geometry.apply(\n#     lambda x: water.geometry.distance(x).min()\n# )\n\n# # Compute distance to nearest park\n# train_gdf[\"distance_to_park\"] = train_gdf.geometry.apply(\n#     lambda x: parks.geometry.distance(x).min()\n# )\n# test_gdf[\"distance_to_park\"] = test_gdf.geometry.apply(\n#     lambda x: parks.geometry.distance(x).min()\n# )\n# train_gdf = train_gdf.to_crs('EPSG:4326')\n# test_gdf  = test_gdf.to_crs('EPSG:4326')","metadata":{"execution":{"iopub.status.busy":"2025-02-27T17:59:11.115220Z","iopub.execute_input":"2025-02-27T17:59:11.115905Z","iopub.status.idle":"2025-02-27T17:59:11.265938Z","shell.execute_reply.started":"2025-02-27T17:59:11.115872Z","shell.execute_reply":"2025-02-27T17:59:11.264853Z"},"papermill":{"duration":0.199568,"end_time":"2025-02-06T12:41:19.419698","exception":false,"start_time":"2025-02-06T12:41:19.220130","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\nimport geopandas as gpd\nimport osmnx as ox\nfrom shapely import geometry as geom\nimport pandas as pd\n\ndef compute_all_spatial_features(gdf_points, water, parks, roads=None, nodes=None, buffer_m=10000):\n    \"\"\"\n    Computes spatial features for each point:\n    \n      Baseline features:\n        - distance_to_water: minimum distance to any water body.\n        - distance_to_park: minimum distance to any park.\n        \n      Buffer‐derived road network metrics (using OSMnx):\n        - avg_street_length_osm: mean road segment length within the buffer.\n        - road_density_osm: total road length within the buffer divided by the buffer area.\n        - node_density_osm: count of road nodes per buffer area.\n        - road_segment_count_osm: count of road segments within the buffer.\n        - road_length_std_osm: standard deviation of road segment lengths within the buffer.\n      \n      Additional Crucial Features:\n        - avg_closeness_centrality_osm: average closeness centrality of road network nodes within the buffer.\n        - vegetation_ratio_osm: fraction of the buffer area covered by parks.\n    \n    All spatial operations are done in EPSG:2263 for meter‐based calculations;\n    the final output is reprojected back to EPSG:4326.\n    \"\"\"\n    print(\"Reprojecting inputs to EPSG:2263...\")\n    # Reproject inputs for meter-based calculations.\n    gdf_points = gdf_points.to_crs(\"EPSG:2263\").copy()\n    water = water.to_crs(\"EPSG:2263\")\n    parks = parks.to_crs(\"EPSG:2263\")\n    \n    print(\"Computing baseline features...\")\n    # Baseline features.\n    gdf_points[\"distance_to_water\"] = gdf_points.geometry.apply(lambda x: water.geometry.distance(x).min())\n    gdf_points[\"distance_to_park\"] = gdf_points.geometry.apply(lambda x: parks.geometry.distance(x).min())\n    \n    print(\"Creating buffer for each point...\")\n    # Create a buffer around each point.\n    gdf_points[\"buffer\"] = gdf_points.geometry.buffer(buffer_m)\n    \n    print(\"Defining NYC polygon...\")\n    # Define NYC polygon for data downloads.\n    nyc_bbox = (-74.01, 40.75, -73.86, 40.88)  # in EPSG:4326\n    nyc_poly = geom.box(*nyc_bbox)\n    \n    # Download road network and nodes if not provided.\n    if roads is None or nodes is None:\n        print(\"Downloading road network and nodes...\")\n        G = ox.graph_from_polygon(nyc_poly, network_type='drive')\n        roads = ox.graph_to_gdfs(G, nodes=False, edges=True).to_crs(\"EPSG:2263\")\n        nodes = ox.graph_to_gdfs(G, nodes=True, edges=False).to_crs(\"EPSG:2263\")\n        print(\"Computing closeness centrality for nodes...\")\n        closeness_dict = nx.closeness_centrality(G)\n        nodes[\"closeness\"] = nodes.index.map(closeness_dict)\n    else:\n        print(\"Using pre-downloaded road network and nodes.\")\n    \n    print(\"Performing spatial joins...\")\n    # Use the buffer geometry for spatial joins.\n    buffer_gdf = gdf_points.copy().set_geometry(\"buffer\")\n    \n    # Join with roads (edges).\n    joined_edges = gpd.sjoin(buffer_gdf, roads, how=\"left\", predicate=\"intersects\")\n    # Join with nodes.\n    joined_nodes = gpd.sjoin(buffer_gdf, nodes, how=\"left\", predicate=\"contains\")\n    # Join with parks (for vegetation cover).\n    buffer_gdf[\"buffer_geom\"] = buffer_gdf.geometry  # preserve buffer geometry.\n    joined_parks = gpd.sjoin(buffer_gdf, parks, how=\"left\", predicate=\"intersects\")\n    # Compute intersection area between each buffer and the park geometry.\n    joined_parks[\"intersection_area\"] = joined_parks.apply(\n        lambda row: row[\"buffer_geom\"].intersection(row[\"geometry\"]).area if row[\"geometry\"] is not None else 0, axis=1)\n    \n    print(\"Computing road network metrics...\")\n    metrics = pd.DataFrame(index=gdf_points.index)\n    \n    # Average street length within the buffer.\n    avg_street_length = joined_edges.groupby(joined_edges.index)[\"length\"].mean()\n    metrics[\"avg_street_length_osm\"] = avg_street_length.reindex(gdf_points.index, fill_value=0)\n    \n    # Road density: total road length divided by buffer area.\n    total_road_length = joined_edges.groupby(joined_edges.index)[\"length\"].sum()\n    metrics[\"road_density_osm\"] = total_road_length.reindex(gdf_points.index, fill_value=0) / gdf_points[\"buffer\"].area\n    \n    # Node density: count of nodes per buffer area.\n    node_count = joined_nodes.groupby(joined_nodes.index).size()\n    metrics[\"node_density_osm\"] = node_count.reindex(gdf_points.index, fill_value=0) / gdf_points[\"buffer\"].area\n    \n    # Road segment count: count of road segments in the buffer.\n    road_segment_count = joined_edges.groupby(joined_edges.index)[\"length\"].count()\n    metrics[\"road_segment_count_osm\"] = road_segment_count.reindex(gdf_points.index, fill_value=0)\n    \n    # Road length standard deviation within the buffer.\n    road_length_std = joined_edges.groupby(joined_edges.index)[\"length\"].std()\n    metrics[\"road_length_std_osm\"] = road_length_std.reindex(gdf_points.index, fill_value=0)\n    \n    print(\"Computing additional crucial features...\")\n    # Additional features.\n    # Average closeness centrality from nodes within the buffer.\n    avg_closeness = joined_nodes.groupby(joined_nodes.index)[\"closeness\"].mean()\n    metrics[\"avg_closeness_centrality_osm\"] = avg_closeness.reindex(gdf_points.index, fill_value=0)\n    \n    # Vegetation ratio: fraction of the buffer area covered by parks.\n    # park_area = joined_parks.groupby(joined_parks.index)[\"intersection_area\"].sum()\n    # vegetation_ratio = park_area.reindex(gdf_points.index, fill_value=0) / gdf_points[\"buffer\"].area\n    # metrics[\"vegetation_ratio_osm\"] = vegetation_ratio\n    \n    print(\"Merging metrics with original data...\")\n    # Merge computed metrics with the original GeoDataFrame.\n    gdf_points = gdf_points.join(metrics)\n    \n    print(\"Dropping temporary buffer column and reprojecting to EPSG:4326...\")\n    # Remove the temporary buffer column and reproject the result.\n    gdf_points.drop(columns=[\"buffer\"], inplace=True)\n    return gdf_points.to_crs(\"EPSG:4326\")\n\n# =============================================================================\n# Example usage:\n# Precompute the expensive datasets once and then pass them in to save computation.\n# Assume train_gdf and test_gdf are GeoDataFrames (with coordinates in EPSG:4326),\n# and water and parks are GeoDataFrames in EPSG:4326.\n# =============================================================================\n# print(\"Precomputing shared data (road network and nodes)...\")\n# nyc_bbox = (-74.01, 40.75, -73.86, 40.88)\n# nyc_poly = geom.box(*nyc_bbox)\n\n# print(\"Downloading road network and nodes for shared data...\")\n# G = ox.graph_from_polygon(nyc_poly, network_type='drive')\n# roads_shared = ox.graph_to_gdfs(G, nodes=False, edges=True).to_crs(\"EPSG:2263\")\n# nodes_shared = ox.graph_to_gdfs(G, nodes=True, edges=False).to_crs(\"EPSG:2263\")\n# closeness_dict = nx.closeness_centrality(G)\n# nodes_shared[\"closeness\"] = nodes_shared.index.map(closeness_dict)\n\n# print(\"Sample of computed features:\")\n# print(train_final[[\"distance_to_water\", \"distance_to_park\",\n#                     \"avg_street_length_osm\", \"road_density_osm\",\n#                     \"node_density_osm\"]].head())\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T17:59:11.266976Z","iopub.execute_input":"2025-02-27T17:59:11.267276Z","iopub.status.idle":"2025-02-27T17:59:11.283206Z","shell.execute_reply.started":"2025-02-27T17:59:11.267250Z","shell.execute_reply":"2025-02-27T17:59:11.281960Z"},"papermill":{"duration":237.769552,"end_time":"2025-02-06T12:45:17.195644","exception":false,"start_time":"2025-02-06T12:41:19.426092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input_cols = [col for col in train_final.columns if col!='Longitude' and col!='Latitude'\n#              and col!='datetime' and col!='geometry' and col!='UHI Index']\n# input_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:59:11.284349Z","iopub.execute_input":"2025-02-27T17:59:11.284666Z","iopub.status.idle":"2025-02-27T17:59:11.309256Z","shell.execute_reply.started":"2025-02-27T17:59:11.284632Z","shell.execute_reply":"2025-02-27T17:59:11.308220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_features(train, test, water, parks, roads, nodes, buffer_list):\n    \"\"\"Processes multiple buffers into unified DataFrames with _[buffer]m suffixes\"\"\"\n    \n    # Initialize merged DataFrames with core columns\n    merged_train = train[['Longitude', 'Latitude', 'datetime', 'UHI Index', 'geometry']].copy()\n    merged_test = test[['Longitude', 'Latitude', 'geometry']].copy()  # Removed UHI Index if test doesn't have it\n\n    for buffer in buffer_list:\n        print(f'Processing buffer: {buffer}m')\n        \n        # Compute features for current buffer\n        train_buffer = compute_all_spatial_features(train, water, parks, roads, nodes, buffer)\n        test_buffer = compute_all_spatial_features(test, water, parks, roads, nodes, buffer)\n        \n        # Identify feature columns (exclude locators/target)\n        feature_cols = [col for col in train_buffer.columns \n                        if col not in ['Longitude', 'Latitude', 'datetime', 'UHI Index', 'geometry']]\n        \n        # Add buffer suffix and merge\n        merged_train = merged_train.join(\n            train_buffer[feature_cols].add_suffix(f'_{buffer}m'))\n        merged_test = merged_test.join(\n            test_buffer[feature_cols].add_suffix(f'_{buffer}m'))\n    \n    # Drop geometry if not needed in final output\n    merged_train = merged_train.drop(columns='geometry')\n    merged_test = merged_test.drop(columns='geometry')\n    \n    # Save final unified datasets\n    merged_train.to_csv('train_unified_buffers.csv', index=False)\n    merged_test.to_csv('test_unified_buffers.csv', index=False)\n    \n    return merged_train, merged_test\n\n\nbuffer_list = [500, 1000, 1500,\n               2000, 2500, 3000, 3500,\n               4000, 4500, 5000, 6000, 7000, 7500, 8000,\n               8500, 9000, 9500, 10000, 10500, 11000] \ntrain_full, test_full = compute_features(train_gdf, test_gdf, water, parks, roads, nodes, buffer_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:15:43.230628Z","iopub.execute_input":"2025-02-27T18:15:43.231093Z","iopub.status.idle":"2025-02-27T18:17:49.382077Z","shell.execute_reply.started":"2025-02-27T18:15:43.231054Z","shell.execute_reply":"2025-02-27T18:17:49.381232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_full.head(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:19:09.788020Z","iopub.execute_input":"2025-02-27T18:19:09.788358Z","iopub.status.idle":"2025-02-27T18:19:09.808367Z","shell.execute_reply.started":"2025-02-27T18:19:09.788334Z","shell.execute_reply":"2025-02-27T18:19:09.807226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_final_1000 = compute_all_spatial_features(train_gdf, water, parks,\n#                                            roads=roads, nodes=nodes, buffer_m=1000)\n# test_final_1000  = compute_all_spatial_features(test_gdf, water, parks,\n#                                           roads=roads, nodes=nodes, buffer_m=1000)\n# train_final_100[input_cols].to_csv()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.247474Z","iopub.status.idle":"2025-02-27T18:02:43.247847Z","shell.execute_reply":"2025-02-27T18:02:43.247711Z"},"papermill":{"duration":0.041436,"end_time":"2025-02-06T12:45:17.243694","exception":false,"start_time":"2025-02-06T12:45:17.202258","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input_cols = ['distance_to_water', 'distance_to_park', 'avg_street_length_osm', 'road_density_osm', 'node_density_osm']","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.248737Z","iopub.status.idle":"2025-02-27T18:02:43.249055Z","shell.execute_reply":"2025-02-27T18:02:43.248929Z"},"papermill":{"duration":0.013933,"end_time":"2025-02-06T12:45:17.264272","exception":false,"start_time":"2025-02-06T12:45:17.250339","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import xgboost as xgb\n# xgb_reg = xgb.XGBRegressor()\n# target = train_final['UHI Index']\n# xgb_reg.fit(train_final[input_cols], target)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.249748Z","iopub.status.idle":"2025-02-27T18:02:43.250222Z","shell.execute_reply":"2025-02-27T18:02:43.249936Z"},"papermill":{"duration":0.479552,"end_time":"2025-02-06T12:45:17.750437","exception":false,"start_time":"2025-02-06T12:45:17.270885","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preds = xgb_reg.predict(test_final[input_cols])","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.251047Z","iopub.status.idle":"2025-02-27T18:02:43.251338Z","shell.execute_reply":"2025-02-27T18:02:43.251221Z"},"papermill":{"duration":0.021008,"end_time":"2025-02-06T12:45:17.778100","exception":false,"start_time":"2025-02-06T12:45:17.757092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_df['UHI Index'] = preds\n# test_df.to_csv('submission_osmnx_features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.252019Z","iopub.status.idle":"2025-02-27T18:02:43.252390Z","shell.execute_reply":"2025-02-27T18:02:43.252257Z"},"papermill":{"duration":0.024772,"end_time":"2025-02-06T12:45:17.811504","exception":false,"start_time":"2025-02-06T12:45:17.786732","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_final[input_cols].to_csv('train_osmnx_milked.csv', index=False)\n# test_final[input_cols].to_csv('test_osmnx_milked.csv'  , index=False  )","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.253227Z","iopub.status.idle":"2025-02-27T18:02:43.253679Z","shell.execute_reply":"2025-02-27T18:02:43.253463Z"},"papermill":{"duration":0.122584,"end_time":"2025-02-06T12:45:17.940847","exception":false,"start_time":"2025-02-06T12:45:17.818263","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_save = ['distance_to_water', 'distance_to_park']\n# train_gdf[cols_to_save].to_csv('train_dis.csv', index=False)\n# test_gdf[cols_to_save].to_csv('test_dis.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.254693Z","iopub.status.idle":"2025-02-27T18:02:43.255156Z","shell.execute_reply":"2025-02-27T18:02:43.254951Z"},"papermill":{"duration":0.013383,"end_time":"2025-02-06T12:45:17.961089","exception":false,"start_time":"2025-02-06T12:45:17.947706","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shapely.geometry as geom\n# import geopandas as gpd\n# def compute_road_density(gdf_points, buffer_m=1500):\n#     \"\"\"\n#     Computes road density (total road length per buffer area) for each point in gdf_points.\n#     Instead of using the convex hull of gdf_points, we use a fixed bounding polygon\n#     for NYC.\n#     \"\"\"\n#     # Define a fixed bounding box for NYC in (min_lon, min_lat, max_lon, max_lat)\n#     nyc_bbox = (-74.01, 40.75, -73.86, 40.88)\n#     # Create a polygon from the bounding box (in lon, lat order, i.e. EPSG:4326)\n#     nyc_poly = geom.box(*nyc_bbox)\n    \n#     # Download the road network within this polygon (driveable roads)\n#     G = ox.graph_from_polygon(nyc_poly, network_type='drive')\n#     roads = ox.graph_to_gdfs(G, nodes=False, edges=True)\n#     # Reproject roads to EPSG:2263 (meters)\n#     roads = roads.to_crs(\"EPSG:2263\")\n    \n#     # Compute road density per point:\n#     # Buffer each point by buffer_m meters.\n#     gdf_points = gdf_points.copy()\n#     gdf_points[\"buffer\"] = gdf_points.geometry.buffer(buffer_m)\n    \n#     # Spatial join: intersect each buffer with the road network.\n#     joined = gpd.sjoin(gdf_points.set_geometry(\"buffer\"), roads, how=\"left\", predicate=\"intersects\")\n    \n#     # For each point, sum the 'length' of road segments that intersect.\n#     # (Ensure that the roads GeoDataFrame has a 'length' column;\n#     # if not, compute it using roads.geometry.length)\n#     if \"length\" not in roads.columns:\n#         roads[\"length\"] = roads.geometry.length\n\n#     road_length = joined.groupby(joined.index)[\"length\"].sum()\n    \n#     # Calculate road density = total road length / area of the buffer.\n#     gdf_points[\"road_density\"] = road_length / gdf_points[\"buffer\"].area\n#     gdf_points[\"road_density\"] = gdf_points[\"road_density\"].fillna(0)\n#     gdf_points.drop(columns=[\"buffer\"], inplace=True)\n    \n#     return gdf_points\n\n\n# # Uncomment if you want to add road density:\n# train_gdf_rd = compute_road_density(train_gdf, buffer_m=1500)\n# test_gdf_rd  = compute_road_density(test_gdf, buffer_m=1500)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.256112Z","iopub.status.idle":"2025-02-27T18:02:43.256704Z","shell.execute_reply":"2025-02-27T18:02:43.256478Z"},"papermill":{"duration":0.013833,"end_time":"2025-02-06T12:45:17.981513","exception":false,"start_time":"2025-02-06T12:45:17.967680","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def compute_osmnx_metrics(gdf_points, buffer_m=1500):\n#     \"\"\"\n#     For each point in gdf_points (in EPSG:2263), creates a buffer of buffer_m meters\n#     and computes:\n#       - avg_street_length: mean road segment length in the buffer\n#       - road_density: total road length per buffer area\n#       - node_density: count of road nodes per buffer area\n#       - avg_node_degree: approximate connectivity, 2*(# edges)/(# nodes)\n#     Returns gdf_points with these new metrics, with suffix '_osm'.\n#     \"\"\"\n#     # Fixed bounding polygon for NYC (in EPSG:4326), then reproject to EPSG:2263.\n#     nyc_bbox = (-74.01, 40.75, -73.86, 40.88)\n#     nyc_poly = geom.box(*nyc_bbox)\n#     nyc_poly_gdf = gpd.GeoDataFrame({'geometry': [nyc_poly]}, crs=\"EPSG:4326\").to_crs(\"EPSG:2263\")\n    \n#     # Download road network for NYC (driveable network)\n#     G = ox.graph_from_polygon(nyc_poly, network_type='drive')\n#     roads = ox.graph_to_gdfs(G, nodes=False, edges=True).to_crs(\"EPSG:2263\")\n#     nodes = ox.graph_to_gdfs(G, nodes=True, edges=False).to_crs(\"EPSG:2263\")\n    \n#     # Create a buffer around each point\n#     gdf_points = gdf_points.copy()\n#     gdf_points[\"buffer\"] = gdf_points.geometry.buffer(buffer_m)\n    \n#     # Spatial join with roads (edges)\n#     joined_edges = gpd.sjoin(gdf_points.set_geometry(\"buffer\"), roads, how=\"left\", predicate=\"intersects\")\n#     # Spatial join with nodes\n#     joined_nodes = gpd.sjoin(gdf_points.set_geometry(\"buffer\"), nodes, how=\"left\", predicate=\"contains\")\n    \n#     # Compute metrics per point in a new DataFrame\n#     metrics = pd.DataFrame(index=gdf_points.index)\n    \n#     # 1. Average Street Length\n#     avg_street_length = joined_edges.groupby(joined_edges.index)[\"length\"].mean()\n#     metrics[\"avg_street_length\"] = avg_street_length.reindex(gdf_points.index, fill_value=0)\n    \n#     # 2. Road Density\n#     total_road_length = joined_edges.groupby(joined_edges.index)[\"length\"].sum()\n#     metrics[\"road_density\"] = total_road_length.reindex(gdf_points.index, fill_value=0) / gdf_points[\"buffer\"].area\n    \n#     # 3. Node Density\n#     node_count = joined_nodes.groupby(joined_nodes.index).size()\n#     metrics[\"node_density\"] = node_count.reindex(gdf_points.index, fill_value=0) / gdf_points[\"buffer\"].area\n    \n#     # 4. Connectivity: approximate as 2 * (# unique edges) / (# unique nodes)\n#     # unique_edges = joined_edges.groupby(joined_edges.index).apply(lambda df: df.index.nunique())\n#     # unique_nodes = joined_nodes.groupby(joined_nodes.index).apply(lambda df: df.index.nunique())\n#     # connectivity = 2 * unique_edges / unique_nodes.replace(0, np.nan)\n#     # connectivity = connectivity.fillna(0)\n#     # metrics[\"avg_node_degree\"] = connectivity.reindex(gdf_points.index, fill_value=0)\n    \n#     # Add a suffix to avoid overlap\n#     metrics = metrics.add_suffix(\"_osm\")\n    \n#     # Remove the temporary buffer column and join metrics\n#     gdf_points = gdf_points.drop(columns=[\"buffer\"])\n#     gdf_points = gdf_points.join(metrics)\n    \n#     return gdf_points\n\n# # Example usage:\n# train_gdf_osmnx_metrics = compute_osmnx_metrics(train_gdf, buffer_m=1500)\n# test_gdf_osmnx_metrics  = compute_osmnx_metrics(test_gdf , buffer_m=1500)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.258522Z","iopub.status.idle":"2025-02-27T18:02:43.259012Z","shell.execute_reply":"2025-02-27T18:02:43.258831Z"},"papermill":{"duration":47.687278,"end_time":"2025-02-06T12:46:05.675552","exception":false,"start_time":"2025-02-06T12:45:17.988274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_gdf_osmnx_metrics.nunique()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.259823Z","iopub.status.idle":"2025-02-27T18:02:43.260347Z","shell.execute_reply":"2025-02-27T18:02:43.260078Z"},"papermill":{"duration":0.020878,"end_time":"2025-02-06T12:46:05.703613","exception":false,"start_time":"2025-02-06T12:46:05.682735","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def compute_intersection_density(gdf_points, buffer_m=1500):\n#     # Define a fixed bounding box for NYC (EPSG:4326) and create a polygon\n#     nyc_bbox = (-74.01, 40.75, -73.86, 40.88)\n#     nyc_poly = geom.box(*nyc_bbox)\n    \n#     # Download the road network nodes (driveable network)\n#     nodes = ox.graph_to_gdfs(ox.graph_from_polygon(nyc_poly, network_type='drive'),\n#                               nodes=True, edges=False)\n#     nodes = nodes.to_crs(\"EPSG:2263\")\n    \n#     # Buffer each point\n#     gdf_points = gdf_points.copy()\n#     gdf_points[\"buffer\"] = gdf_points.geometry.buffer(buffer_m)\n    \n#     # Spatial join: assign nodes (intersections) to each buffered point\n#     joined = gpd.sjoin(gdf_points.set_geometry(\"buffer\"), nodes, how=\"left\", predicate=\"contains\")\n    \n#     # Count intersections per point, and calculate density (intersections per square meter)\n#     intersection_counts = joined.groupby(joined.index).size()\n#     gdf_points[\"intersection_density\"] = intersection_counts.reindex(gdf_points.index, fill_value=0).values / gdf_points[\"buffer\"].area\n    \n#     gdf_points.drop(columns=[\"buffer\"], inplace=True)\n#     return gdf_points\n\n# # Example usage:\n# # Assuming train_gdf and test_gdf are your GeoDataFrames in EPSG:2263:\n# train_gdf_intersection = compute_intersection_density(train_gdf, buffer_m=1500)\n# test_gdf_intersection = compute_intersection_density(test_gdf, buffer_m=1500)","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.262040Z","iopub.status.idle":"2025-02-27T18:02:43.262585Z","shell.execute_reply":"2025-02-27T18:02:43.262368Z"},"papermill":{"duration":43.153478,"end_time":"2025-02-06T12:46:48.863633","exception":false,"start_time":"2025-02-06T12:46:05.710155","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_gdf_intersection.nunique()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.263968Z","iopub.status.idle":"2025-02-27T18:02:43.264485Z","shell.execute_reply":"2025-02-27T18:02:43.264256Z"},"papermill":{"duration":0.020279,"end_time":"2025-02-06T12:46:48.891034","exception":false,"start_time":"2025-02-06T12:46:48.870755","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install rasterio\n# !wget https://s3.amazonaws.com/elevation-tiles-prod/skadi/N40/N40W074.hgt.gz\n# !gunzip N40W074.hgt.gz","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.265321Z","iopub.status.idle":"2025-02-27T18:02:43.265798Z","shell.execute_reply":"2025-02-27T18:02:43.265582Z"},"papermill":{"duration":0.014448,"end_time":"2025-02-06T12:46:48.912221","exception":false,"start_time":"2025-02-06T12:46:48.897773","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import rasterio\n# from rasterio.transform import from_origin\n\n# # NYC Approximate Boundaries\n# bounds = (-74.3, 40.5, -73.7, 40.9)\n\n# # Create DEM GeoTIFF from HGT file\n# with rasterio.open('/kaggle/working/N40W074.hgt') as src:\n#     dem_data = src.read(1)\n#     transform = from_origin(src.bounds.left, src.bounds.top, src.res[0], src.res[1])\n#     profile = src.profile\n\n#     profile.update({\n#         'driver': 'GTiff',\n#         'height': dem_data.shape[0],\n#         'width': dem_data.shape[1],\n#         'transform': transform,\n#         'crs': 'EPSG:4326'\n#     })\n\n#     with rasterio.open('nyc_dem.tif', 'w', **profile) as dst:\n#         dst.write(dem_data, 1)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.267148Z","iopub.status.idle":"2025-02-27T18:02:43.267640Z","shell.execute_reply":"2025-02-27T18:02:43.267432Z"},"papermill":{"duration":0.013548,"end_time":"2025-02-06T12:46:48.932709","exception":false,"start_time":"2025-02-06T12:46:48.919161","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import rasterio\n# from rasterio.enums import Resampling\n# from scipy.ndimage import sobel, generic_filter\n\n# # Function to extract elevation and additional features\n# def extract_elevation_features(df, dem_path='nyc_dem_2263.tif'):\n#     with rasterio.open(dem_path) as dem:\n#         band = dem.read(1, resampling=Resampling.bilinear)\n#         nodata = dem.nodata\n\n#         # Replace nodata values with np.nan for clean processing\n#         band = np.where(band == nodata, np.nan, band)\n\n#         # Elevation Extraction\n#         def get_elevation(lat, lon):\n#             try:\n#                 row, col = dem.index(lon, lat)\n#                 if 0 <= row < band.shape[0] and 0 <= col < band.shape[1]:\n#                     return band[row, col]\n#                 else:\n#                     return np.nan\n#             except IndexError:\n#                 return np.nan\n\n#         # Slope Calculation\n#         def calculate_slope(band):\n#             # Replace NaNs temporarily to avoid sqrt issues\n#             safe_band = np.nan_to_num(band, nan=0)\n#             dx = sobel(safe_band, axis=0, mode='constant')\n#             dy = sobel(safe_band, axis=1, mode='constant')\n#             slope = np.sqrt(dx**2 + dy**2)\n#             slope[np.isnan(band)] = np.nan  # Restore NaNs\n#             return slope\n\n#         # Aspect Calculation\n#         def calculate_aspect(dx, dy):\n#             aspect = np.arctan2(-dx, dy)\n#             aspect = np.degrees(aspect)\n#             aspect = np.where(aspect < 0, 360 + aspect, aspect)\n#             return np.where(np.isnan(dx) | np.isnan(dy), np.nan, aspect)\n\n#         # Ruggedness Calculation\n#         def calculate_ruggedness(band, window=3):\n#             return generic_filter(band, np.nanstd, size=window)\n\n#         # Apply elevation\n#         df['elevation'] = df.apply(lambda x: get_elevation(x['Latitude'], x['Longitude']), axis=1)\n\n#         # Derive slope, aspect, ruggedness\n#         slope = calculate_slope(band)\n#         dx = sobel(np.nan_to_num(band, nan=0), axis=0, mode='constant')\n#         dy = sobel(np.nan_to_num(band, nan=0), axis=1, mode='constant')\n#         aspect = calculate_aspect(dx, dy)\n#         ruggedness = calculate_ruggedness(band)\n\n#         # Map features to coordinates\n#         def map_to_coords(lat, lon, feature_band):\n#             try:\n#                 row, col = dem.index(lon, lat)\n#                 if 0 <= row < feature_band.shape[0] and 0 <= col < feature_band.shape[1]:\n#                     return feature_band[row, col]\n#                 else:\n#                     return np.nan\n#             except IndexError:\n#                 return np.nan\n\n#         df['slope'] = df.apply(lambda x: map_to_coords(x['Latitude'], x['Longitude'], slope), axis=1)\n#         df['aspect'] = df.apply(lambda x: map_to_coords(x['Latitude'], x['Longitude'], aspect), axis=1)\n#         df['ruggedness'] = df.apply(lambda x: map_to_coords(x['Latitude'], x['Longitude'], ruggedness), axis=1)\n\n#     return df\n\n# # Apply to train and test sets\n# train_df = extract_elevation_features(train_df.iloc[:20])\n# test_df = extract_elevation_features(test_df.iloc[:20])\n\n# # Check results\n# print(train_df[['Latitude', 'Longitude', 'elevation', 'slope', 'aspect', 'ruggedness']].head())\n# print(test_df[['Latitude', 'Longitude', 'elevation', 'slope', 'aspect', 'ruggedness']].head())\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.268740Z","iopub.status.idle":"2025-02-27T18:02:43.269261Z","shell.execute_reply":"2025-02-27T18:02:43.269039Z"},"papermill":{"duration":0.01366,"end_time":"2025-02-06T12:46:48.953377","exception":false,"start_time":"2025-02-06T12:46:48.939717","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T18:02:43.270288Z","iopub.status.idle":"2025-02-27T18:02:43.270808Z","shell.execute_reply":"2025-02-27T18:02:43.270561Z"},"papermill":{"duration":0.012709,"end_time":"2025-02-06T12:46:48.972993","exception":false,"start_time":"2025-02-06T12:46:48.960284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006404,"end_time":"2025-02-06T12:46:48.986266","exception":false,"start_time":"2025-02-06T12:46:48.979862","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}
