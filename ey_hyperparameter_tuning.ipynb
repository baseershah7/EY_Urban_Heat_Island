{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10580423,"sourceType":"datasetVersion","datasetId":6547867},{"sourceId":10580457,"sourceType":"datasetVersion","datasetId":6547892},{"sourceId":10672463,"sourceType":"datasetVersion","datasetId":6610449},{"sourceId":10759536,"sourceType":"datasetVersion","datasetId":6674058},{"sourceId":224881021,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport optuna \nimport xgboost as xgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import r2_score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:14.879182Z","iopub.execute_input":"2025-02-27T22:33:14.879407Z","iopub.status.idle":"2025-02-27T22:33:17.096238Z","shell.execute_reply.started":"2025-02-27T22:33:14.879385Z","shell.execute_reply":"2025-02-27T22:33:17.095214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/uhi-index-data/Training_data_uhi_index_UHI2025-v2.csv')\ntarget = train_df['UHI Index']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:17.097238Z","iopub.execute_input":"2025-02-27T22:33:17.097791Z","iopub.status.idle":"2025-02-27T22:33:17.132280Z","shell.execute_reply.started":"2025-02-27T22:33:17.097753Z","shell.execute_reply":"2025-02-27T22:33:17.131380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# n_clusters = 50\n# coords = train_df[['Latitude', 'Longitude']].values\n# kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n# train_df['group'] = kmeans.fit_predict(coords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:17.133104Z","iopub.execute_input":"2025-02-27T22:33:17.133337Z","iopub.status.idle":"2025-02-27T22:33:17.137059Z","shell.execute_reply.started":"2025-02-27T22:33:17.133315Z","shell.execute_reply":"2025-02-27T22:33:17.136173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = pd.read_csv('/kaggle/input/ey-final-boss/ey_train_niche_ds_0.982.csv')\ny = train_df['UHI Index']  # Ensure train_df is defined and contains 'UHI Index'\ncols = X.columns\nX.fillna(0, inplace=True)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_scaled = pd.DataFrame(X_scaled, columns=[cols])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:35:28.694679Z","iopub.execute_input":"2025-02-27T22:35:28.695037Z","iopub.status.idle":"2025-02-27T22:35:28.904493Z","shell.execute_reply.started":"2025-02-27T22:35:28.695014Z","shell.execute_reply":"2025-02-27T22:35:28.903735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# optuna","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:17.399173Z","iopub.execute_input":"2025-02-27T22:33:17.399557Z","iopub.status.idle":"2025-02-27T22:33:17.403890Z","shell.execute_reply.started":"2025-02-27T22:33:17.399519Z","shell.execute_reply":"2025-02-27T22:33:17.402914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef objective(trial):\n    # Intense parameter grid for ExtraTreesRegressor\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 5000),\n        'max_depth': trial.suggest_int('max_depth', 10, 100),  # depth can go quite deep\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None, 0.3, 0.5, 0.7]),\n        'bootstrap': trial.suggest_categorical('bootstrap', [False]),\n        'criterion': trial.suggest_categorical('criterion', ['squared_error']),\n    }\n    \n    # If bootstrap is True, consider tuning max_samples as well\n    if params['bootstrap']:\n        params['max_samples'] = trial.suggest_uniform('max_samples', 0.5, 1.0)\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.24, random_state=42)\n    \n    model = ExtraTreesRegressor(**params, n_jobs=-1, random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    score = r2_score(y_val, preds)\n    return score\n\n# Create and run the study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=800)\n\nprint('Best parameters:', study.best_params)\nprint('Best CV R2 score:', study.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:35:50.938298Z","iopub.execute_input":"2025-02-27T22:35:50.938658Z","execution_failed":"2025-02-27T22:36:55.577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# import numpy as np\n# import xgboost as xgb\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import r2_score\n\n# def objective(trial):\n#     # Define parameter ranges around your current best values\n#     params = {\n#         'objective': 'reg:squarederror',\n#         'booster': 'gbtree',\n#         'tree_method': 'gpu_hist',  # Use \"hist\" for speed; change to \"gpu_hist\" if you have GPU support\n#         'n_estimators': trial.suggest_int('n_estimators', 2000, 6500),  # around 4200\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.09),  # around 0.035\n#         'max_depth': trial.suggest_int('max_depth', 7, 30),  # around 17\n#         'min_child_weight': trial.suggest_int('min_child_weight', 3, 25),  # around 15\n#         'subsample': trial.suggest_uniform('subsample', 0.65, 1.0),  # around 0.9\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.65, 1.0),  # around 0.9\n#         'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.65, 0.9),  # around 0.8\n#  #       'gamma': trial.suggest_loguniform('gamma', 1e-9, 5.0),  # minimum loss reduction required to make a split\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-9, 1.0),  # L1 regularization\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1.9, 2.0),  # L2 regularization\n#         'n_jobs': -1,\n#         'enable_categorical': False,\n#     }\n\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.24, random_state=42)\n    \n#     model = xgb.XGBRegressor(**params, random_state=42, verbosity=0)\n#     model.fit(X_train, y_train)\n#     preds = model.predict(X_val)\n#     score = r2_score(y_val, preds)\n#     return score\n\n# # Create and run the study\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=400)\n\n# print('Best parameters :', study.best_params)\n# print('Best CV R2 score :', study.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:17.716144Z","iopub.status.idle":"2025-02-27T22:33:17.716447Z","shell.execute_reply":"2025-02-27T22:33:17.716305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# import numpy as np\n# import lightgbm as lgb\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import r2_score\n\n# def objective(trial):\n#     params = {\n#         'objective': 'regression',\n#         'metric': 'rmse',               # metric for training; we'll compute R2 later\n#         'boosting_type': 'gbdt',\n#         'device': 'gpu',                # Use GPU\n#         'gpu_platform_id': 0,           # Default GPU platform id (adjust if needed)\n#         'gpu_device_id': 0,             # Default GPU device id (adjust if needed)\n#         'n_estimators': trial.suggest_int('n_estimators', 2000, 6500),  # around 4200\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.09),  # around 0.035\n#         'max_depth': trial.suggest_int('max_depth', 7, 25),   # around 17\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 25),  # similar to min_child_weight=15\n#         'subsample': trial.suggest_uniform('subsample', 0.65, 1.0),           # around 0.9\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.65, 1.0),  # around 0.9\n#         'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.65, 1.0), # around 0.8\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),         # L1 regularization\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1.0, 2.0),          # L2 regularization\n#         'n_jobs': -1,\n#         'verbose': -1\n#     }\n    \n#     # Create a holdout split: 80% training, 20% validation.\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n#     train_data = lgb.Dataset(X_train, label=y_train)\n#     val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n    \n#     model = lgb.train(params, train_data, valid_sets=[val_data])\n    \n#     preds = model.predict(X_val)\n#     score = r2_score(y_val, preds)\n#     return score\n\n# # Create an Optuna study (direction is 'maximize' because higher R2 is better)\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=400)\n\n# print('Best parameters :', study.best_params)\n# print('Best CV R2 score :', study.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T22:33:17.717284Z","iopub.status.idle":"2025-02-27T22:33:17.717704Z","shell.execute_reply":"2025-02-27T22:33:17.717511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}